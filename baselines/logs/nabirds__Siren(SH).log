/home/zl22853/code/GeoBS/baselines/main.py:149: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)
  lats, lons = np.radians(loc_tr[:,1]), np.radians(loc_tr[:,0])
GPU Name: NVIDIA RTX A6000
Loading nabirds_with_loc_2019.json - train
   using meta data: ebird_meta
	 22819 total entries
	 22819 entries with images
	 22599 entries with meta data
Loading nabirds_with_loc_2019.json - test
   using meta data: ebird_meta
	 24633 total entries
	 24633 entries with images
	 24411 entries with meta data
Check the radian of input data! tensor([-117,   33])
Training for 50 epochs.
[epoch 1, batch    30] loss: 6.295
[epoch 1, batch    60] loss: 6.164
[epoch 1, batch    90] loss: 5.575
[epoch 1, batch   120] loss: 4.487
[epoch 1, batch   150] loss: 3.216
[epoch 1, batch   180] loss: 2.336
[epoch 1, batch   210] loss: 1.678
[epoch 1, batch   240] loss: 1.415
[epoch 1, batch   270] loss: 1.196
[epoch 1, batch   300] loss: 1.135
[epoch 1, batch   330] loss: 0.893
[epoch 1, batch   360] loss: 0.988
[epoch 1, batch   390] loss: 0.889
[epoch 1, batch   420] loss: 0.776
[epoch 1, batch   450] loss: 0.713
[epoch 1, batch   480] loss: 0.798
[epoch 1, batch   510] loss: 0.683
[epoch 1, batch   540] loss: 0.652
[epoch 1, batch   570] loss: 0.664
[epoch 1, batch   600] loss: 0.593
[epoch 1, batch   630] loss: 0.655
[epoch 1, batch   660] loss: 0.628
[epoch 1, batch   690] loss: 0.459
epoch 1 mean loss: 1.8361
[epoch 2, batch    30] loss: 0.525
[epoch 2, batch    60] loss: 0.459
[epoch 2, batch    90] loss: 0.368
[epoch 2, batch   120] loss: 0.483
[epoch 2, batch   150] loss: 0.400
[epoch 2, batch   180] loss: 0.447
[epoch 2, batch   210] loss: 0.469
[epoch 2, batch   240] loss: 0.342
[epoch 2, batch   270] loss: 0.478
[epoch 2, batch   300] loss: 0.418
[epoch 2, batch   330] loss: 0.405
[epoch 2, batch   360] loss: 0.422
[epoch 2, batch   390] loss: 0.465
[epoch 2, batch   420] loss: 0.470
[epoch 2, batch   450] loss: 0.343
[epoch 2, batch   480] loss: 0.470
[epoch 2, batch   510] loss: 0.436
[epoch 2, batch   540] loss: 0.375
[epoch 2, batch   570] loss: 0.330
[epoch 2, batch   600] loss: 0.373
[epoch 2, batch   630] loss: 0.334
[epoch 2, batch   660] loss: 0.390
[epoch 2, batch   690] loss: 0.366
epoch 2 mean loss: 0.4182
[epoch 3, batch    30] loss: 0.330
[epoch 3, batch    60] loss: 0.330
[epoch 3, batch    90] loss: 0.285
[epoch 3, batch   120] loss: 0.238
[epoch 3, batch   150] loss: 0.286
[epoch 3, batch   180] loss: 0.291
[epoch 3, batch   210] loss: 0.293
[epoch 3, batch   240] loss: 0.323
[epoch 3, batch   270] loss: 0.307
[epoch 3, batch   300] loss: 0.302
[epoch 3, batch   330] loss: 0.320
[epoch 3, batch   360] loss: 0.295
[epoch 3, batch   390] loss: 0.411
[epoch 3, batch   420] loss: 0.346
[epoch 3, batch   450] loss: 0.310
[epoch 3, batch   480] loss: 0.280
[epoch 3, batch   510] loss: 0.311
[epoch 3, batch   540] loss: 0.342
[epoch 3, batch   570] loss: 0.359
[epoch 3, batch   600] loss: 0.299
[epoch 3, batch   630] loss: 0.300
[epoch 3, batch   660] loss: 0.310
[epoch 3, batch   690] loss: 0.303
epoch 3 mean loss: 0.3116
[epoch 4, batch    30] loss: 0.165
[epoch 4, batch    60] loss: 0.259
[epoch 4, batch    90] loss: 0.199
[epoch 4, batch   120] loss: 0.232
[epoch 4, batch   150] loss: 0.245
[epoch 4, batch   180] loss: 0.206
[epoch 4, batch   210] loss: 0.271
[epoch 4, batch   240] loss: 0.242
[epoch 4, batch   270] loss: 0.209
[epoch 4, batch   300] loss: 0.183
[epoch 4, batch   330] loss: 0.248
[epoch 4, batch   360] loss: 0.237
[epoch 4, batch   390] loss: 0.171
[epoch 4, batch   420] loss: 0.141
[epoch 4, batch   450] loss: 0.311
[epoch 4, batch   480] loss: 0.224
[epoch 4, batch   510] loss: 0.224
[epoch 4, batch   540] loss: 0.220
[epoch 4, batch   570] loss: 0.283
[epoch 4, batch   600] loss: 0.221
[epoch 4, batch   630] loss: 0.296
[epoch 4, batch   660] loss: 0.234
[epoch 4, batch   690] loss: 0.217
epoch 4 mean loss: 0.2291
[epoch 5, batch    30] loss: 0.199
[epoch 5, batch    60] loss: 0.155
[epoch 5, batch    90] loss: 0.207
[epoch 5, batch   120] loss: 0.148
[epoch 5, batch   150] loss: 0.189
[epoch 5, batch   180] loss: 0.209
[epoch 5, batch   210] loss: 0.188
[epoch 5, batch   240] loss: 0.182
[epoch 5, batch   270] loss: 0.242
[epoch 5, batch   300] loss: 0.269
[epoch 5, batch   330] loss: 0.145
[epoch 5, batch   360] loss: 0.221
[epoch 5, batch   390] loss: 0.229
[epoch 5, batch   420] loss: 0.239
[epoch 5, batch   450] loss: 0.280
[epoch 5, batch   480] loss: 0.174
[epoch 5, batch   510] loss: 0.232
[epoch 5, batch   540] loss: 0.244
[epoch 5, batch   570] loss: 0.217
[epoch 5, batch   600] loss: 0.183
[epoch 5, batch   630] loss: 0.276
[epoch 5, batch   660] loss: 0.247
[epoch 5, batch   690] loss: 0.212
epoch 5 mean loss: 0.2140
[epoch 6, batch    30] loss: 0.189
[epoch 6, batch    60] loss: 0.115
[epoch 6, batch    90] loss: 0.148
[epoch 6, batch   120] loss: 0.185
[epoch 6, batch   150] loss: 0.190
[epoch 6, batch   180] loss: 0.201
[epoch 6, batch   210] loss: 0.189
[epoch 6, batch   240] loss: 0.165
[epoch 6, batch   270] loss: 0.171
[epoch 6, batch   300] loss: 0.110
[epoch 6, batch   330] loss: 0.138
[epoch 6, batch   360] loss: 0.161
[epoch 6, batch   390] loss: 0.166
[epoch 6, batch   420] loss: 0.185
[epoch 6, batch   450] loss: 0.198
[epoch 6, batch   480] loss: 0.157
[epoch 6, batch   510] loss: 0.137
[epoch 6, batch   540] loss: 0.179
[epoch 6, batch   570] loss: 0.158
[epoch 6, batch   600] loss: 0.314
[epoch 6, batch   630] loss: 0.189
[epoch 6, batch   660] loss: 0.142
[epoch 6, batch   690] loss: 0.197
epoch 6 mean loss: 0.1718
[epoch 7, batch    30] loss: 0.143
[epoch 7, batch    60] loss: 0.146
[epoch 7, batch    90] loss: 0.165
[epoch 7, batch   120] loss: 0.115
[epoch 7, batch   150] loss: 0.143
[epoch 7, batch   180] loss: 0.124
[epoch 7, batch   210] loss: 0.169
[epoch 7, batch   240] loss: 0.154
[epoch 7, batch   270] loss: 0.123
[epoch 7, batch   300] loss: 0.169
[epoch 7, batch   330] loss: 0.132
[epoch 7, batch   360] loss: 0.117
[epoch 7, batch   390] loss: 0.143
[epoch 7, batch   420] loss: 0.132
[epoch 7, batch   450] loss: 0.166
[epoch 7, batch   480] loss: 0.132
[epoch 7, batch   510] loss: 0.134
[epoch 7, batch   540] loss: 0.178
[epoch 7, batch   570] loss: 0.180
[epoch 7, batch   600] loss: 0.224
[epoch 7, batch   630] loss: 0.180
[epoch 7, batch   660] loss: 0.162
[epoch 7, batch   690] loss: 0.167
epoch 7 mean loss: 0.1543
[epoch 8, batch    30] loss: 0.130
[epoch 8, batch    60] loss: 0.133
[epoch 8, batch    90] loss: 0.140
[epoch 8, batch   120] loss: 0.170
[epoch 8, batch   150] loss: 0.102
[epoch 8, batch   180] loss: 0.141
[epoch 8, batch   210] loss: 0.151
[epoch 8, batch   240] loss: 0.131
[epoch 8, batch   270] loss: 0.098
[epoch 8, batch   300] loss: 0.153
[epoch 8, batch   330] loss: 0.138
[epoch 8, batch   360] loss: 0.096
[epoch 8, batch   390] loss: 0.113
[epoch 8, batch   420] loss: 0.150
[epoch 8, batch   450] loss: 0.116
[epoch 8, batch   480] loss: 0.088
[epoch 8, batch   510] loss: 0.103
[epoch 8, batch   540] loss: 0.142
[epoch 8, batch   570] loss: 0.126
[epoch 8, batch   600] loss: 0.151
[epoch 8, batch   630] loss: 0.123
[epoch 8, batch   660] loss: 0.128
[epoch 8, batch   690] loss: 0.115
epoch 8 mean loss: 0.1277
[epoch 9, batch    30] loss: 0.107
[epoch 9, batch    60] loss: 0.144
[epoch 9, batch    90] loss: 0.140
[epoch 9, batch   120] loss: 0.113
[epoch 9, batch   150] loss: 0.125
[epoch 9, batch   180] loss: 0.111
[epoch 9, batch   210] loss: 0.094
[epoch 9, batch   240] loss: 0.083
[epoch 9, batch   270] loss: 0.094
[epoch 9, batch   300] loss: 0.117
[epoch 9, batch   330] loss: 0.155
[epoch 9, batch   360] loss: 0.124
[epoch 9, batch   390] loss: 0.102
[epoch 9, batch   420] loss: 0.115
[epoch 9, batch   450] loss: 0.098
[epoch 9, batch   480] loss: 0.126
[epoch 9, batch   510] loss: 0.076
[epoch 9, batch   540] loss: 0.090
[epoch 9, batch   570] loss: 0.119
[epoch 9, batch   600] loss: 0.130
[epoch 9, batch   630] loss: 0.141
[epoch 9, batch   660] loss: 0.111
[epoch 9, batch   690] loss: 0.086
epoch 9 mean loss: 0.1136
[epoch 10, batch    30] loss: 0.162
[epoch 10, batch    60] loss: 0.114
[epoch 10, batch    90] loss: 0.063
[epoch 10, batch   120] loss: 0.078
[epoch 10, batch   150] loss: 0.091
[epoch 10, batch   180] loss: 0.111
[epoch 10, batch   210] loss: 0.083
[epoch 10, batch   240] loss: 0.129
[epoch 10, batch   270] loss: 0.081
[epoch 10, batch   300] loss: 0.146
[epoch 10, batch   330] loss: 0.115
[epoch 10, batch   360] loss: 0.072
[epoch 10, batch   390] loss: 0.120
[epoch 10, batch   420] loss: 0.155
[epoch 10, batch   450] loss: 0.134
[epoch 10, batch   480] loss: 0.125
[epoch 10, batch   510] loss: 0.137
[epoch 10, batch   540] loss: 0.119
[epoch 10, batch   570] loss: 0.110
[epoch 10, batch   600] loss: 0.104
[epoch 10, batch   630] loss: 0.107
[epoch 10, batch   660] loss: 0.097
[epoch 10, batch   690] loss: 0.122
epoch 10 mean loss: 0.1117
[epoch 11, batch    30] loss: 0.086
[epoch 11, batch    60] loss: 0.099
[epoch 11, batch    90] loss: 0.072
[epoch 11, batch   120] loss: 0.055
[epoch 11, batch   150] loss: 0.104
[epoch 11, batch   180] loss: 0.106
[epoch 11, batch   210] loss: 0.083
[epoch 11, batch   240] loss: 0.054
[epoch 11, batch   270] loss: 0.136
[epoch 11, batch   300] loss: 0.085
[epoch 11, batch   330] loss: 0.072
[epoch 11, batch   360] loss: 0.134
[epoch 11, batch   390] loss: 0.105
[epoch 11, batch   420] loss: 0.076
[epoch 11, batch   450] loss: 0.124
[epoch 11, batch   480] loss: 0.129
[epoch 11, batch   510] loss: 0.092
[epoch 11, batch   540] loss: 0.072
[epoch 11, batch   570] loss: 0.058
[epoch 11, batch   600] loss: 0.098
[epoch 11, batch   630] loss: 0.080
[epoch 11, batch   660] loss: 0.075
[epoch 11, batch   690] loss: 0.120
epoch 11 mean loss: 0.0930
[epoch 12, batch    30] loss: 0.079
[epoch 12, batch    60] loss: 0.076
[epoch 12, batch    90] loss: 0.086
[epoch 12, batch   120] loss: 0.052
[epoch 12, batch   150] loss: 0.075
[epoch 12, batch   180] loss: 0.052
[epoch 12, batch   210] loss: 0.119
[epoch 12, batch   240] loss: 0.101
[epoch 12, batch   270] loss: 0.085
[epoch 12, batch   300] loss: 0.083
[epoch 12, batch   330] loss: 0.068
[epoch 12, batch   360] loss: 0.085
[epoch 12, batch   390] loss: 0.130
[epoch 12, batch   420] loss: 0.114
[epoch 12, batch   450] loss: 0.136
[epoch 12, batch   480] loss: 0.087
[epoch 12, batch   510] loss: 0.116
[epoch 12, batch   540] loss: 0.145
[epoch 12, batch   570] loss: 0.106
[epoch 12, batch   600] loss: 0.109
[epoch 12, batch   630] loss: 0.075
[epoch 12, batch   660] loss: 0.109
[epoch 12, batch   690] loss: 0.102
epoch 12 mean loss: 0.0956
[epoch 13, batch    30] loss: 0.075
[epoch 13, batch    60] loss: 0.053
[epoch 13, batch    90] loss: 0.097
[epoch 13, batch   120] loss: 0.095
[epoch 13, batch   150] loss: 0.140
[epoch 13, batch   180] loss: 0.099
[epoch 13, batch   210] loss: 0.098
[epoch 13, batch   240] loss: 0.049
[epoch 13, batch   270] loss: 0.102
[epoch 13, batch   300] loss: 0.113
[epoch 13, batch   330] loss: 0.061
[epoch 13, batch   360] loss: 0.090
[epoch 13, batch   390] loss: 0.077
[epoch 13, batch   420] loss: 0.109
[epoch 13, batch   450] loss: 0.071
[epoch 13, batch   480] loss: 0.087
[epoch 13, batch   510] loss: 0.066
[epoch 13, batch   540] loss: 0.073
[epoch 13, batch   570] loss: 0.052
[epoch 13, batch   600] loss: 0.078
[epoch 13, batch   630] loss: 0.101
[epoch 13, batch   660] loss: 0.121
[epoch 13, batch   690] loss: 0.082
epoch 13 mean loss: 0.0858
[epoch 14, batch    30] loss: 0.137
[epoch 14, batch    60] loss: 0.100
[epoch 14, batch    90] loss: 0.087
[epoch 14, batch   120] loss: 0.072
[epoch 14, batch   150] loss: 0.142
[epoch 14, batch   180] loss: 0.114
[epoch 14, batch   210] loss: 0.065
[epoch 14, batch   240] loss: 0.082
[epoch 14, batch   270] loss: 0.069
[epoch 14, batch   300] loss: 0.052
[epoch 14, batch   330] loss: 0.103
[epoch 14, batch   360] loss: 0.089
[epoch 14, batch   390] loss: 0.080
[epoch 14, batch   420] loss: 0.055
[epoch 14, batch   450] loss: 0.068
[epoch 14, batch   480] loss: 0.080
[epoch 14, batch   510] loss: 0.096
[epoch 14, batch   540] loss: 0.069
[epoch 14, batch   570] loss: 0.094
[epoch 14, batch   600] loss: 0.071
[epoch 14, batch   630] loss: 0.070
[epoch 14, batch   660] loss: 0.129
[epoch 14, batch   690] loss: 0.093
epoch 14 mean loss: 0.0870
[epoch 15, batch    30] loss: 0.061
[epoch 15, batch    60] loss: 0.052
[epoch 15, batch    90] loss: 0.085
[epoch 15, batch   120] loss: 0.077
[epoch 15, batch   150] loss: 0.066
[epoch 15, batch   180] loss: 0.070
[epoch 15, batch   210] loss: 0.085
[epoch 15, batch   240] loss: 0.081
[epoch 15, batch   270] loss: 0.079
[epoch 15, batch   300] loss: 0.104
[epoch 15, batch   330] loss: 0.091
[epoch 15, batch   360] loss: 0.057
[epoch 15, batch   390] loss: 0.103
[epoch 15, batch   420] loss: 0.102
[epoch 15, batch   450] loss: 0.081
[epoch 15, batch   480] loss: 0.053
[epoch 15, batch   510] loss: 0.128
[epoch 15, batch   540] loss: 0.087
[epoch 15, batch   570] loss: 0.098
[epoch 15, batch   600] loss: 0.084
[epoch 15, batch   630] loss: 0.098
[epoch 15, batch   660] loss: 0.065
[epoch 15, batch   690] loss: 0.077
epoch 15 mean loss: 0.0811
[epoch 16, batch    30] loss: 0.118
[epoch 16, batch    60] loss: 0.078
[epoch 16, batch    90] loss: 0.051
[epoch 16, batch   120] loss: 0.064
[epoch 16, batch   150] loss: 0.047
[epoch 16, batch   180] loss: 0.078
[epoch 16, batch   210] loss: 0.060
[epoch 16, batch   240] loss: 0.143
[epoch 16, batch   270] loss: 0.081
[epoch 16, batch   300] loss: 0.081
[epoch 16, batch   330] loss: 0.089
[epoch 16, batch   360] loss: 0.129
[epoch 16, batch   390] loss: 0.044
[epoch 16, batch   420] loss: 0.096
[epoch 16, batch   450] loss: 0.052
[epoch 16, batch   480] loss: 0.065
[epoch 16, batch   510] loss: 0.098
[epoch 16, batch   540] loss: 0.064
[epoch 16, batch   570] loss: 0.081
[epoch 16, batch   600] loss: 0.118
[epoch 16, batch   630] loss: 0.058
[epoch 16, batch   660] loss: 0.109
[epoch 16, batch   690] loss: 0.066
epoch 16 mean loss: 0.0808
[epoch 17, batch    30] loss: 0.066
[epoch 17, batch    60] loss: 0.065
[epoch 17, batch    90] loss: 0.095
[epoch 17, batch   120] loss: 0.085
[epoch 17, batch   150] loss: 0.046
[epoch 17, batch   180] loss: 0.066
[epoch 17, batch   210] loss: 0.079
[epoch 17, batch   240] loss: 0.118
[epoch 17, batch   270] loss: 0.099
[epoch 17, batch   300] loss: 0.094
[epoch 17, batch   330] loss: 0.097
[epoch 17, batch   360] loss: 0.072
[epoch 17, batch   390] loss: 0.040
[epoch 17, batch   420] loss: 0.079
[epoch 17, batch   450] loss: 0.080
[epoch 17, batch   480] loss: 0.138
[epoch 17, batch   510] loss: 0.095
[epoch 17, batch   540] loss: 0.091
[epoch 17, batch   570] loss: 0.099
[epoch 17, batch   600] loss: 0.057
[epoch 17, batch   630] loss: 0.107
[epoch 17, batch   660] loss: 0.064
[epoch 17, batch   690] loss: 0.049
epoch 17 mean loss: 0.0827
[epoch 18, batch    30] loss: 0.041
[epoch 18, batch    60] loss: 0.058
[epoch 18, batch    90] loss: 0.104
[epoch 18, batch   120] loss: 0.083
[epoch 18, batch   150] loss: 0.098
[epoch 18, batch   180] loss: 0.054
[epoch 18, batch   210] loss: 0.056
[epoch 18, batch   240] loss: 0.084
[epoch 18, batch   270] loss: 0.081
[epoch 18, batch   300] loss: 0.036
[epoch 18, batch   330] loss: 0.047
[epoch 18, batch   360] loss: 0.092
[epoch 18, batch   390] loss: 0.074
[epoch 18, batch   420] loss: 0.080
[epoch 18, batch   450] loss: 0.046
[epoch 18, batch   480] loss: 0.060
[epoch 18, batch   510] loss: 0.055
[epoch 18, batch   540] loss: 0.056
[epoch 18, batch   570] loss: 0.100
[epoch 18, batch   600] loss: 0.059
[epoch 18, batch   630] loss: 0.091
[epoch 18, batch   660] loss: 0.043
[epoch 18, batch   690] loss: 0.089
epoch 18 mean loss: 0.0685
[epoch 19, batch    30] loss: 0.066
[epoch 19, batch    60] loss: 0.064
[epoch 19, batch    90] loss: 0.061
[epoch 19, batch   120] loss: 0.070
[epoch 19, batch   150] loss: 0.104
[epoch 19, batch   180] loss: 0.045
[epoch 19, batch   210] loss: 0.099
[epoch 19, batch   240] loss: 0.080
[epoch 19, batch   270] loss: 0.074
[epoch 19, batch   300] loss: 0.133
[epoch 19, batch   330] loss: 0.053
[epoch 19, batch   360] loss: 0.074
[epoch 19, batch   390] loss: 0.081
[epoch 19, batch   420] loss: 0.121
[epoch 19, batch   450] loss: 0.152
[epoch 19, batch   480] loss: 0.044
[epoch 19, batch   510] loss: 0.080
[epoch 19, batch   540] loss: 0.065
[epoch 19, batch   570] loss: 0.075
[epoch 19, batch   600] loss: 0.056
[epoch 19, batch   630] loss: 0.097
[epoch 19, batch   660] loss: 0.049
[epoch 19, batch   690] loss: 0.076
epoch 19 mean loss: 0.0809
[epoch 20, batch    30] loss: 0.052
[epoch 20, batch    60] loss: 0.075
[epoch 20, batch    90] loss: 0.030
[epoch 20, batch   120] loss: 0.047
[epoch 20, batch   150] loss: 0.076
[epoch 20, batch   180] loss: 0.127
[epoch 20, batch   210] loss: 0.068
[epoch 20, batch   240] loss: 0.083
[epoch 20, batch   270] loss: 0.093
[epoch 20, batch   300] loss: 0.093
[epoch 20, batch   330] loss: 0.074
[epoch 20, batch   360] loss: 0.084
[epoch 20, batch   390] loss: 0.062
[epoch 20, batch   420] loss: 0.053
[epoch 20, batch   450] loss: 0.060
[epoch 20, batch   480] loss: 0.027
[epoch 20, batch   510] loss: 0.074
[epoch 20, batch   540] loss: 0.053
[epoch 20, batch   570] loss: 0.083
[epoch 20, batch   600] loss: 0.080
[epoch 20, batch   630] loss: 0.085
[epoch 20, batch   660] loss: 0.057
[epoch 20, batch   690] loss: 0.091
epoch 20 mean loss: 0.0712
[epoch 21, batch    30] loss: 0.069
[epoch 21, batch    60] loss: 0.083
[epoch 21, batch    90] loss: 0.063
[epoch 21, batch   120] loss: 0.069
[epoch 21, batch   150] loss: 0.071
[epoch 21, batch   180] loss: 0.065
[epoch 21, batch   210] loss: 0.073
[epoch 21, batch   240] loss: 0.057
[epoch 21, batch   270] loss: 0.088
[epoch 21, batch   300] loss: 0.064
[epoch 21, batch   330] loss: 0.061
[epoch 21, batch   360] loss: 0.045
[epoch 21, batch   390] loss: 0.060
[epoch 21, batch   420] loss: 0.064
[epoch 21, batch   450] loss: 0.068
[epoch 21, batch   480] loss: 0.053
[epoch 21, batch   510] loss: 0.047
[epoch 21, batch   540] loss: 0.023
[epoch 21, batch   570] loss: 0.041
[epoch 21, batch   600] loss: 0.047
[epoch 21, batch   630] loss: 0.048
[epoch 21, batch   660] loss: 0.087
[epoch 21, batch   690] loss: 0.083
epoch 21 mean loss: 0.0621
[epoch 22, batch    30] loss: 0.066
[epoch 22, batch    60] loss: 0.047
[epoch 22, batch    90] loss: 0.072
[epoch 22, batch   120] loss: 0.061
[epoch 22, batch   150] loss: 0.044
[epoch 22, batch   180] loss: 0.077
[epoch 22, batch   210] loss: 0.084
[epoch 22, batch   240] loss: 0.080
[epoch 22, batch   270] loss: 0.047
[epoch 22, batch   300] loss: 0.058
[epoch 22, batch   330] loss: 0.077
[epoch 22, batch   360] loss: 0.092
[epoch 22, batch   390] loss: 0.068
[epoch 22, batch   420] loss: 0.057
[epoch 22, batch   450] loss: 0.095
[epoch 22, batch   480] loss: 0.090
[epoch 22, batch   510] loss: 0.089
[epoch 22, batch   540] loss: 0.058
[epoch 22, batch   570] loss: 0.057
[epoch 22, batch   600] loss: 0.055
[epoch 22, batch   630] loss: 0.062
[epoch 22, batch   660] loss: 0.053
[epoch 22, batch   690] loss: 0.054
epoch 22 mean loss: 0.0664
[epoch 23, batch    30] loss: 0.047
[epoch 23, batch    60] loss: 0.073
[epoch 23, batch    90] loss: 0.033
[epoch 23, batch   120] loss: 0.062
[epoch 23, batch   150] loss: 0.052
[epoch 23, batch   180] loss: 0.057
[epoch 23, batch   210] loss: 0.039
[epoch 23, batch   240] loss: 0.039
[epoch 23, batch   270] loss: 0.055
[epoch 23, batch   300] loss: 0.041
[epoch 23, batch   330] loss: 0.077
[epoch 23, batch   360] loss: 0.040
[epoch 23, batch   390] loss: 0.058
[epoch 23, batch   420] loss: 0.050
[epoch 23, batch   450] loss: 0.045
[epoch 23, batch   480] loss: 0.069
[epoch 23, batch   510] loss: 0.071
[epoch 23, batch   540] loss: 0.090
[epoch 23, batch   570] loss: 0.081
[epoch 23, batch   600] loss: 0.111
[epoch 23, batch   630] loss: 0.057
[epoch 23, batch   660] loss: 0.035
[epoch 23, batch   690] loss: 0.034
epoch 23 mean loss: 0.0573
[epoch 24, batch    30] loss: 0.049
[epoch 24, batch    60] loss: 0.075
[epoch 24, batch    90] loss: 0.055
[epoch 24, batch   120] loss: 0.052
[epoch 24, batch   150] loss: 0.114
[epoch 24, batch   180] loss: 0.049
[epoch 24, batch   210] loss: 0.064
[epoch 24, batch   240] loss: 0.069
[epoch 24, batch   270] loss: 0.033
[epoch 24, batch   300] loss: 0.032
[epoch 24, batch   330] loss: 0.066
[epoch 24, batch   360] loss: 0.060
[epoch 24, batch   390] loss: 0.044
[epoch 24, batch   420] loss: 0.067
[epoch 24, batch   450] loss: 0.062
[epoch 24, batch   480] loss: 0.037
[epoch 24, batch   510] loss: 0.065
[epoch 24, batch   540] loss: 0.047
[epoch 24, batch   570] loss: 0.114
[epoch 24, batch   600] loss: 0.050
[epoch 24, batch   630] loss: 0.069
[epoch 24, batch   660] loss: 0.043
[epoch 24, batch   690] loss: 0.113
epoch 24 mean loss: 0.0625
[epoch 25, batch    30] loss: 0.060
[epoch 25, batch    60] loss: 0.080
[epoch 25, batch    90] loss: 0.051
[epoch 25, batch   120] loss: 0.081
[epoch 25, batch   150] loss: 0.061
[epoch 25, batch   180] loss: 0.049
[epoch 25, batch   210] loss: 0.105
[epoch 25, batch   240] loss: 0.121
[epoch 25, batch   270] loss: 0.090
[epoch 25, batch   300] loss: 0.051
[epoch 25, batch   330] loss: 0.048
[epoch 25, batch   360] loss: 0.073
[epoch 25, batch   390] loss: 0.055
[epoch 25, batch   420] loss: 0.042
[epoch 25, batch   450] loss: 0.095
[epoch 25, batch   480] loss: 0.093
[epoch 25, batch   510] loss: 0.084
[epoch 25, batch   540] loss: 0.056
[epoch 25, batch   570] loss: 0.053
[epoch 25, batch   600] loss: 0.043
[epoch 25, batch   630] loss: 0.059
[epoch 25, batch   660] loss: 0.044
[epoch 25, batch   690] loss: 0.067
epoch 25 mean loss: 0.0693
[epoch 26, batch    30] loss: 0.031
[epoch 26, batch    60] loss: 0.033
[epoch 26, batch    90] loss: 0.091
[epoch 26, batch   120] loss: 0.043
[epoch 26, batch   150] loss: 0.035
[epoch 26, batch   180] loss: 0.049
[epoch 26, batch   210] loss: 0.054
[epoch 26, batch   240] loss: 0.041
[epoch 26, batch   270] loss: 0.093
[epoch 26, batch   300] loss: 0.181
[epoch 26, batch   330] loss: 0.091
[epoch 26, batch   360] loss: 0.046
[epoch 26, batch   390] loss: 0.040
[epoch 26, batch   420] loss: 0.052
[epoch 26, batch   450] loss: 0.072
[epoch 26, batch   480] loss: 0.039
[epoch 26, batch   510] loss: 0.051
[epoch 26, batch   540] loss: 0.038
[epoch 26, batch   570] loss: 0.050
[epoch 26, batch   600] loss: 0.046
[epoch 26, batch   630] loss: 0.055
[epoch 26, batch   660] loss: 0.035
[epoch 26, batch   690] loss: 0.065
epoch 26 mean loss: 0.0575
[epoch 27, batch    30] loss: 0.078
[epoch 27, batch    60] loss: 0.036
[epoch 27, batch    90] loss: 0.045
[epoch 27, batch   120] loss: 0.024
[epoch 27, batch   150] loss: 0.030
[epoch 27, batch   180] loss: 0.026
[epoch 27, batch   210] loss: 0.021
[epoch 27, batch   240] loss: 0.022
[epoch 27, batch   270] loss: 0.040
[epoch 27, batch   300] loss: 0.042
[epoch 27, batch   330] loss: 0.012
[epoch 27, batch   360] loss: 0.035
[epoch 27, batch   390] loss: 0.025
[epoch 27, batch   420] loss: 0.040
[epoch 27, batch   450] loss: 0.087
[epoch 27, batch   480] loss: 0.021
[epoch 27, batch   510] loss: 0.029
[epoch 27, batch   540] loss: 0.039
[epoch 27, batch   570] loss: 0.030
[epoch 27, batch   600] loss: 0.009
[epoch 27, batch   630] loss: 0.041
[epoch 27, batch   660] loss: 0.017
[epoch 27, batch   690] loss: 0.056
epoch 27 mean loss: 0.0351
[epoch 28, batch    30] loss: 0.025
[epoch 28, batch    60] loss: 0.053
[epoch 28, batch    90] loss: 0.023
[epoch 28, batch   120] loss: 0.029
[epoch 28, batch   150] loss: 0.028
[epoch 28, batch   180] loss: 0.037
[epoch 28, batch   210] loss: 0.026
[epoch 28, batch   240] loss: 0.020
[epoch 28, batch   270] loss: 0.010
[epoch 28, batch   300] loss: 0.035
[epoch 28, batch   330] loss: 0.011
[epoch 28, batch   360] loss: 0.027
[epoch 28, batch   390] loss: 0.014
[epoch 28, batch   420] loss: 0.036
[epoch 28, batch   450] loss: 0.013
[epoch 28, batch   480] loss: 0.010
[epoch 28, batch   510] loss: 0.012
[epoch 28, batch   540] loss: 0.003
[epoch 28, batch   570] loss: 0.024
[epoch 28, batch   600] loss: 0.011
[epoch 28, batch   630] loss: 0.017
[epoch 28, batch   660] loss: 0.036
[epoch 28, batch   690] loss: 0.026
epoch 28 mean loss: 0.0228
[epoch 29, batch    30] loss: 0.033
[epoch 29, batch    60] loss: 0.027
[epoch 29, batch    90] loss: 0.020
[epoch 29, batch   120] loss: 0.021
[epoch 29, batch   150] loss: 0.012
[epoch 29, batch   180] loss: 0.021
[epoch 29, batch   210] loss: 0.008
[epoch 29, batch   240] loss: 0.019
[epoch 29, batch   270] loss: 0.015
[epoch 29, batch   300] loss: 0.043
[epoch 29, batch   330] loss: 0.029
[epoch 29, batch   360] loss: 0.021
[epoch 29, batch   390] loss: 0.018
[epoch 29, batch   420] loss: 0.011
[epoch 29, batch   450] loss: 0.024
[epoch 29, batch   480] loss: 0.021
[epoch 29, batch   510] loss: 0.035
[epoch 29, batch   540] loss: 0.025
[epoch 29, batch   570] loss: 0.022
[epoch 29, batch   600] loss: 0.012
[epoch 29, batch   630] loss: 0.009
[epoch 29, batch   660] loss: 0.024
[epoch 29, batch   690] loss: 0.026
epoch 29 mean loss: 0.0214
[epoch 30, batch    30] loss: 0.006
[epoch 30, batch    60] loss: 0.003
[epoch 30, batch    90] loss: 0.019
[epoch 30, batch   120] loss: 0.009
[epoch 30, batch   150] loss: 0.034
[epoch 30, batch   180] loss: 0.042
[epoch 30, batch   210] loss: 0.019
[epoch 30, batch   240] loss: 0.008
[epoch 30, batch   270] loss: 0.011
[epoch 30, batch   300] loss: 0.012
[epoch 30, batch   330] loss: 0.022
[epoch 30, batch   360] loss: 0.009
[epoch 30, batch   390] loss: 0.028
[epoch 30, batch   420] loss: 0.029
[epoch 30, batch   450] loss: 0.016
[epoch 30, batch   480] loss: 0.006
[epoch 30, batch   510] loss: 0.008
[epoch 30, batch   540] loss: 0.012
[epoch 30, batch   570] loss: 0.013
[epoch 30, batch   600] loss: 0.042
[epoch 30, batch   630] loss: 0.026
[epoch 30, batch   660] loss: 0.024
[epoch 30, batch   690] loss: 0.009
epoch 30 mean loss: 0.0184
[epoch 31, batch    30] loss: 0.028
[epoch 31, batch    60] loss: 0.023
[epoch 31, batch    90] loss: 0.011
[epoch 31, batch   120] loss: 0.018
[epoch 31, batch   150] loss: 0.005
[epoch 31, batch   180] loss: 0.008
[epoch 31, batch   210] loss: 0.016
[epoch 31, batch   240] loss: 0.044
[epoch 31, batch   270] loss: 0.028
[epoch 31, batch   300] loss: 0.018
[epoch 31, batch   330] loss: 0.011
[epoch 31, batch   360] loss: 0.042
[epoch 31, batch   390] loss: 0.004
[epoch 31, batch   420] loss: 0.015
[epoch 31, batch   450] loss: 0.011
[epoch 31, batch   480] loss: 0.035
[epoch 31, batch   510] loss: 0.028
[epoch 31, batch   540] loss: 0.019
[epoch 31, batch   570] loss: 0.032
[epoch 31, batch   600] loss: 0.013
[epoch 31, batch   630] loss: 0.006
[epoch 31, batch   660] loss: 0.018
[epoch 31, batch   690] loss: 0.012
epoch 31 mean loss: 0.0195
[epoch 32, batch    30] loss: 0.007
[epoch 32, batch    60] loss: 0.007
[epoch 32, batch    90] loss: 0.026
[epoch 32, batch   120] loss: 0.013
[epoch 32, batch   150] loss: 0.022
[epoch 32, batch   180] loss: 0.013
[epoch 32, batch   210] loss: 0.015
[epoch 32, batch   240] loss: 0.015
[epoch 32, batch   270] loss: 0.016
[epoch 32, batch   300] loss: 0.022
[epoch 32, batch   330] loss: 0.030
[epoch 32, batch   360] loss: 0.023
[epoch 32, batch   390] loss: 0.024
[epoch 32, batch   420] loss: 0.012
[epoch 32, batch   450] loss: 0.008
[epoch 32, batch   480] loss: 0.006
[epoch 32, batch   510] loss: 0.026
[epoch 32, batch   540] loss: 0.031
[epoch 32, batch   570] loss: 0.005
[epoch 32, batch   600] loss: 0.005
[epoch 32, batch   630] loss: 0.017
[epoch 32, batch   660] loss: 0.025
[epoch 32, batch   690] loss: 0.021
epoch 32 mean loss: 0.0167
[epoch 33, batch    30] loss: 0.033
[epoch 33, batch    60] loss: 0.013
[epoch 33, batch    90] loss: 0.005
[epoch 33, batch   120] loss: 0.020
[epoch 33, batch   150] loss: 0.014
[epoch 33, batch   180] loss: 0.040
[epoch 33, batch   210] loss: 0.036
[epoch 33, batch   240] loss: 0.025
[epoch 33, batch   270] loss: 0.008
[epoch 33, batch   300] loss: 0.011
[epoch 33, batch   330] loss: 0.018
[epoch 33, batch   360] loss: 0.026
[epoch 33, batch   390] loss: 0.024
[epoch 33, batch   420] loss: 0.022
[epoch 33, batch   450] loss: 0.012
[epoch 33, batch   480] loss: 0.015
[epoch 33, batch   510] loss: 0.022
[epoch 33, batch   540] loss: 0.007
[epoch 33, batch   570] loss: 0.023
[epoch 33, batch   600] loss: 0.024
[epoch 33, batch   630] loss: 0.018
[epoch 33, batch   660] loss: 0.013
[epoch 33, batch   690] loss: 0.030
epoch 33 mean loss: 0.0199
[epoch 34, batch    30] loss: 0.033
[epoch 34, batch    60] loss: 0.002
[epoch 34, batch    90] loss: 0.017
[epoch 34, batch   120] loss: 0.014
[epoch 34, batch   150] loss: 0.059
[epoch 34, batch   180] loss: 0.015
[epoch 34, batch   210] loss: 0.030
[epoch 34, batch   240] loss: 0.022
[epoch 34, batch   270] loss: 0.020
[epoch 34, batch   300] loss: 0.018
[epoch 34, batch   330] loss: 0.019
[epoch 34, batch   360] loss: 0.056
[epoch 34, batch   390] loss: 0.036
[epoch 34, batch   420] loss: 0.014
[epoch 34, batch   450] loss: 0.007
[epoch 34, batch   480] loss: 0.007
[epoch 34, batch   510] loss: 0.036
[epoch 34, batch   540] loss: 0.005
[epoch 34, batch   570] loss: 0.020
[epoch 34, batch   600] loss: 0.027
[epoch 34, batch   630] loss: 0.019
[epoch 34, batch   660] loss: 0.023
[epoch 34, batch   690] loss: 0.036
epoch 34 mean loss: 0.0229
[epoch 35, batch    30] loss: 0.011
[epoch 35, batch    60] loss: 0.029
[epoch 35, batch    90] loss: 0.005
[epoch 35, batch   120] loss: 0.015
[epoch 35, batch   150] loss: 0.016
[epoch 35, batch   180] loss: 0.002
[epoch 35, batch   210] loss: 0.020
[epoch 35, batch   240] loss: 0.037
[epoch 35, batch   270] loss: 0.007
[epoch 35, batch   300] loss: 0.013
[epoch 35, batch   330] loss: 0.005
[epoch 35, batch   360] loss: 0.013
[epoch 35, batch   390] loss: 0.010
[epoch 35, batch   420] loss: 0.028
[epoch 35, batch   450] loss: 0.038
[epoch 35, batch   480] loss: 0.048
[epoch 35, batch   510] loss: 0.006
[epoch 35, batch   540] loss: 0.008
[epoch 35, batch   570] loss: 0.020
[epoch 35, batch   600] loss: 0.016
[epoch 35, batch   630] loss: 0.031
[epoch 35, batch   660] loss: 0.003
[epoch 35, batch   690] loss: 0.030
epoch 35 mean loss: 0.0183
[epoch 36, batch    30] loss: 0.019
[epoch 36, batch    60] loss: 0.025
[epoch 36, batch    90] loss: 0.003
[epoch 36, batch   120] loss: 0.044
[epoch 36, batch   150] loss: 0.011
[epoch 36, batch   180] loss: 0.017
[epoch 36, batch   210] loss: 0.008
[epoch 36, batch   240] loss: 0.015
[epoch 36, batch   270] loss: 0.036
[epoch 36, batch   300] loss: 0.006
[epoch 36, batch   330] loss: 0.023
[epoch 36, batch   360] loss: 0.007
[epoch 36, batch   390] loss: 0.006
[epoch 36, batch   420] loss: 0.020
[epoch 36, batch   450] loss: 0.006
[epoch 36, batch   480] loss: 0.006
[epoch 36, batch   510] loss: 0.005
[epoch 36, batch   540] loss: 0.031
[epoch 36, batch   570] loss: 0.007
[epoch 36, batch   600] loss: 0.008
[epoch 36, batch   630] loss: 0.036
[epoch 36, batch   660] loss: 0.004
[epoch 36, batch   690] loss: 0.000
epoch 36 mean loss: 0.0145
[epoch 37, batch    30] loss: 0.013
[epoch 37, batch    60] loss: 0.033
[epoch 37, batch    90] loss: 0.025
[epoch 37, batch   120] loss: 0.015
[epoch 37, batch   150] loss: 0.015
[epoch 37, batch   180] loss: 0.037
[epoch 37, batch   210] loss: 0.001
[epoch 37, batch   240] loss: 0.013
[epoch 37, batch   270] loss: 0.007
[epoch 37, batch   300] loss: 0.008
[epoch 37, batch   330] loss: 0.009
[epoch 37, batch   360] loss: 0.013
[epoch 37, batch   390] loss: 0.002
[epoch 37, batch   420] loss: 0.002
[epoch 37, batch   450] loss: 0.014
[epoch 37, batch   480] loss: 0.005
[epoch 37, batch   510] loss: 0.016
[epoch 37, batch   540] loss: 0.010
[epoch 37, batch   570] loss: 0.002
[epoch 37, batch   600] loss: 0.008
[epoch 37, batch   630] loss: 0.001
[epoch 37, batch   660] loss: 0.016
[epoch 37, batch   690] loss: 0.013
epoch 37 mean loss: 0.0118
[epoch 38, batch    30] loss: 0.003
[epoch 38, batch    60] loss: 0.016
[epoch 38, batch    90] loss: 0.026
[epoch 38, batch   120] loss: 0.020
[epoch 38, batch   150] loss: 0.021
[epoch 38, batch   180] loss: 0.019
[epoch 38, batch   210] loss: 0.003
[epoch 38, batch   240] loss: 0.013
[epoch 38, batch   270] loss: 0.003
[epoch 38, batch   300] loss: 0.026
[epoch 38, batch   330] loss: 0.001
[epoch 38, batch   360] loss: 0.002
[epoch 38, batch   390] loss: 0.002
[epoch 38, batch   420] loss: 0.004
[epoch 38, batch   450] loss: 0.010
[epoch 38, batch   480] loss: 0.003
[epoch 38, batch   510] loss: 0.009
[epoch 38, batch   540] loss: 0.004
[epoch 38, batch   570] loss: 0.003
[epoch 38, batch   600] loss: 0.007
[epoch 38, batch   630] loss: 0.007
[epoch 38, batch   660] loss: 0.000
[epoch 38, batch   690] loss: 0.015
epoch 38 mean loss: 0.0093
[epoch 39, batch    30] loss: 0.009
[epoch 39, batch    60] loss: 0.003
[epoch 39, batch    90] loss: 0.007
[epoch 39, batch   120] loss: 0.003
[epoch 39, batch   150] loss: 0.005
[epoch 39, batch   180] loss: 0.012
[epoch 39, batch   210] loss: 0.009
[epoch 39, batch   240] loss: 0.007
[epoch 39, batch   270] loss: 0.009
[epoch 39, batch   300] loss: 0.005
[epoch 39, batch   330] loss: 0.006
[epoch 39, batch   360] loss: 0.003
[epoch 39, batch   390] loss: 0.014
[epoch 39, batch   420] loss: 0.006
[epoch 39, batch   450] loss: 0.016
[epoch 39, batch   480] loss: 0.003
[epoch 39, batch   510] loss: 0.016
[epoch 39, batch   540] loss: 0.006
[epoch 39, batch   570] loss: 0.009
[epoch 39, batch   600] loss: 0.006
[epoch 39, batch   630] loss: 0.003
[epoch 39, batch   660] loss: 0.003
[epoch 39, batch   690] loss: 0.004
epoch 39 mean loss: 0.0071
[epoch 40, batch    30] loss: 0.003
[epoch 40, batch    60] loss: 0.012
[epoch 40, batch    90] loss: 0.006
[epoch 40, batch   120] loss: 0.002
[epoch 40, batch   150] loss: 0.004
[epoch 40, batch   180] loss: 0.001
[epoch 40, batch   210] loss: 0.016
[epoch 40, batch   240] loss: 0.004
[epoch 40, batch   270] loss: 0.000
[epoch 40, batch   300] loss: 0.032
[epoch 40, batch   330] loss: 0.025
[epoch 40, batch   360] loss: 0.013
[epoch 40, batch   390] loss: 0.009
[epoch 40, batch   420] loss: 0.009
[epoch 40, batch   450] loss: 0.001
[epoch 40, batch   480] loss: 0.020
[epoch 40, batch   510] loss: 0.001
[epoch 40, batch   540] loss: 0.002
[epoch 40, batch   570] loss: 0.012
[epoch 40, batch   600] loss: 0.011
[epoch 40, batch   630] loss: 0.036
[epoch 40, batch   660] loss: 0.004
[epoch 40, batch   690] loss: 0.014
epoch 40 mean loss: 0.0104
[epoch 41, batch    30] loss: 0.083
[epoch 41, batch    60] loss: 0.010
[epoch 41, batch    90] loss: 0.001
[epoch 41, batch   120] loss: 0.003
[epoch 41, batch   150] loss: 0.003
[epoch 41, batch   180] loss: 0.007
[epoch 41, batch   210] loss: 0.011
[epoch 41, batch   240] loss: 0.000
[epoch 41, batch   270] loss: 0.003
[epoch 41, batch   300] loss: 0.010
[epoch 41, batch   330] loss: 0.024
[epoch 41, batch   360] loss: 0.017
[epoch 41, batch   390] loss: 0.003
[epoch 41, batch   420] loss: 0.004
[epoch 41, batch   450] loss: 0.005
[epoch 41, batch   480] loss: 0.022
[epoch 41, batch   510] loss: 0.015
[epoch 41, batch   540] loss: 0.023
[epoch 41, batch   570] loss: 0.018
[epoch 41, batch   600] loss: 0.019
[epoch 41, batch   630] loss: 0.003
[epoch 41, batch   660] loss: 0.012
[epoch 41, batch   690] loss: 0.034
epoch 41 mean loss: 0.0144
[epoch 42, batch    30] loss: 0.004
[epoch 42, batch    60] loss: 0.013
[epoch 42, batch    90] loss: 0.011
[epoch 42, batch   120] loss: 0.003
[epoch 42, batch   150] loss: 0.000
[epoch 42, batch   180] loss: 0.013
[epoch 42, batch   210] loss: 0.008
[epoch 42, batch   240] loss: 0.015
[epoch 42, batch   270] loss: 0.008
[epoch 42, batch   300] loss: 0.020
[epoch 42, batch   330] loss: 0.001
[epoch 42, batch   360] loss: 0.003
[epoch 42, batch   390] loss: 0.003
[epoch 42, batch   420] loss: 0.008
[epoch 42, batch   450] loss: 0.002
[epoch 42, batch   480] loss: 0.027
[epoch 42, batch   510] loss: 0.004
[epoch 42, batch   540] loss: 0.005
[epoch 42, batch   570] loss: 0.010
[epoch 42, batch   600] loss: 0.012
[epoch 42, batch   630] loss: 0.011
[epoch 42, batch   660] loss: 0.009
[epoch 42, batch   690] loss: 0.006
epoch 42 mean loss: 0.0087
[epoch 43, batch    30] loss: 0.009
[epoch 43, batch    60] loss: 0.051
[epoch 43, batch    90] loss: 0.003
[epoch 43, batch   120] loss: 0.007
[epoch 43, batch   150] loss: 0.013
[epoch 43, batch   180] loss: 0.014
[epoch 43, batch   210] loss: 0.006
[epoch 43, batch   240] loss: 0.005
[epoch 43, batch   270] loss: 0.000
[epoch 43, batch   300] loss: 0.001
[epoch 43, batch   330] loss: 0.000
[epoch 43, batch   360] loss: 0.008
[epoch 43, batch   390] loss: 0.004
[epoch 43, batch   420] loss: 0.004
[epoch 43, batch   450] loss: 0.001
[epoch 43, batch   480] loss: 0.009
[epoch 43, batch   510] loss: 0.002
[epoch 43, batch   540] loss: 0.001
[epoch 43, batch   570] loss: 0.001
[epoch 43, batch   600] loss: 0.001
[epoch 43, batch   630] loss: 0.001
[epoch 43, batch   660] loss: 0.013
[epoch 43, batch   690] loss: 0.016
epoch 43 mean loss: 0.0073
[epoch 44, batch    30] loss: 0.006
[epoch 44, batch    60] loss: 0.000
[epoch 44, batch    90] loss: 0.000
[epoch 44, batch   120] loss: 0.003
[epoch 44, batch   150] loss: 0.007
[epoch 44, batch   180] loss: 0.007
[epoch 44, batch   210] loss: 0.007
[epoch 44, batch   240] loss: 0.002
[epoch 44, batch   270] loss: 0.000
[epoch 44, batch   300] loss: 0.000
[epoch 44, batch   330] loss: 0.018
[epoch 44, batch   360] loss: 0.001
[epoch 44, batch   390] loss: 0.000
[epoch 44, batch   420] loss: 0.008
[epoch 44, batch   450] loss: 0.003
[epoch 44, batch   480] loss: 0.012
[epoch 44, batch   510] loss: 0.001
[epoch 44, batch   540] loss: 0.006
[epoch 44, batch   570] loss: 0.008
[epoch 44, batch   600] loss: 0.002
[epoch 44, batch   630] loss: 0.004
[epoch 44, batch   660] loss: 0.012
[epoch 44, batch   690] loss: 0.000
epoch 44 mean loss: 0.0047
[epoch 45, batch    30] loss: 0.013
[epoch 45, batch    60] loss: 0.001
[epoch 45, batch    90] loss: 0.008
[epoch 45, batch   120] loss: 0.003
[epoch 45, batch   150] loss: 0.007
[epoch 45, batch   180] loss: 0.003
[epoch 45, batch   210] loss: 0.024
[epoch 45, batch   240] loss: 0.034
[epoch 45, batch   270] loss: 0.001
[epoch 45, batch   300] loss: 0.003
[epoch 45, batch   330] loss: 0.004
[epoch 45, batch   360] loss: 0.000
[epoch 45, batch   390] loss: 0.001
[epoch 45, batch   420] loss: 0.001
[epoch 45, batch   450] loss: 0.001
[epoch 45, batch   480] loss: 0.002
[epoch 45, batch   510] loss: 0.010
[epoch 45, batch   540] loss: 0.001
[epoch 45, batch   570] loss: 0.012
[epoch 45, batch   600] loss: 0.000
[epoch 45, batch   630] loss: 0.002
[epoch 45, batch   660] loss: 0.002
[epoch 45, batch   690] loss: 0.007
epoch 45 mean loss: 0.0060
[epoch 46, batch    30] loss: 0.004
[epoch 46, batch    60] loss: 0.000
[epoch 46, batch    90] loss: 0.001
[epoch 46, batch   120] loss: 0.001
[epoch 46, batch   150] loss: 0.001
[epoch 46, batch   180] loss: 0.000
[epoch 46, batch   210] loss: 0.024
[epoch 46, batch   240] loss: 0.003
[epoch 46, batch   270] loss: 0.004
[epoch 46, batch   300] loss: 0.002
[epoch 46, batch   330] loss: 0.010
[epoch 46, batch   360] loss: 0.000
[epoch 46, batch   390] loss: 0.005
[epoch 46, batch   420] loss: 0.008
[epoch 46, batch   450] loss: 0.018
[epoch 46, batch   480] loss: 0.004
[epoch 46, batch   510] loss: 0.001
[epoch 46, batch   540] loss: 0.006
[epoch 46, batch   570] loss: 0.001
[epoch 46, batch   600] loss: 0.006
[epoch 46, batch   630] loss: 0.018
[epoch 46, batch   660] loss: 0.007
[epoch 46, batch   690] loss: 0.003
epoch 46 mean loss: 0.0055
[epoch 47, batch    30] loss: 0.001
[epoch 47, batch    60] loss: 0.007
[epoch 47, batch    90] loss: 0.004
[epoch 47, batch   120] loss: 0.008
[epoch 47, batch   150] loss: 0.036
[epoch 47, batch   180] loss: 0.001
[epoch 47, batch   210] loss: 0.005
[epoch 47, batch   240] loss: 0.000
[epoch 47, batch   270] loss: 0.002
[epoch 47, batch   300] loss: 0.001
[epoch 47, batch   330] loss: 0.009
[epoch 47, batch   360] loss: 0.006
[epoch 47, batch   390] loss: 0.008
[epoch 47, batch   420] loss: 0.007
[epoch 47, batch   450] loss: 0.004
[epoch 47, batch   480] loss: 0.005
[epoch 47, batch   510] loss: 0.010
[epoch 47, batch   540] loss: 0.002
[epoch 47, batch   570] loss: 0.001
[epoch 47, batch   600] loss: 0.000
[epoch 47, batch   630] loss: 0.002
[epoch 47, batch   660] loss: 0.020
[epoch 47, batch   690] loss: 0.004
epoch 47 mean loss: 0.0062
[epoch 48, batch    30] loss: 0.012
[epoch 48, batch    60] loss: 0.002
[epoch 48, batch    90] loss: 0.003
[epoch 48, batch   120] loss: 0.003
[epoch 48, batch   150] loss: 0.001
[epoch 48, batch   180] loss: 0.012
[epoch 48, batch   210] loss: 0.006
[epoch 48, batch   240] loss: 0.012
[epoch 48, batch   270] loss: 0.006
[epoch 48, batch   300] loss: 0.004
[epoch 48, batch   330] loss: 0.002
[epoch 48, batch   360] loss: 0.002
[epoch 48, batch   390] loss: 0.000
[epoch 48, batch   420] loss: 0.005
[epoch 48, batch   450] loss: 0.002
[epoch 48, batch   480] loss: 0.000
[epoch 48, batch   510] loss: 0.002
[epoch 48, batch   540] loss: 0.005
[epoch 48, batch   570] loss: 0.003
[epoch 48, batch   600] loss: 0.000
[epoch 48, batch   630] loss: 0.001
[epoch 48, batch   660] loss: 0.001
[epoch 48, batch   690] loss: 0.001
epoch 48 mean loss: 0.0037
[epoch 49, batch    30] loss: 0.005
[epoch 49, batch    60] loss: 0.000
[epoch 49, batch    90] loss: 0.009
[epoch 49, batch   120] loss: 0.004
[epoch 49, batch   150] loss: 0.000
[epoch 49, batch   180] loss: 0.019
[epoch 49, batch   210] loss: 0.003
[epoch 49, batch   240] loss: 0.001
[epoch 49, batch   270] loss: 0.000
[epoch 49, batch   300] loss: 0.006
[epoch 49, batch   330] loss: 0.017
[epoch 49, batch   360] loss: 0.008
[epoch 49, batch   390] loss: 0.005
[epoch 49, batch   420] loss: 0.020
[epoch 49, batch   450] loss: 0.008
[epoch 49, batch   480] loss: 0.000
[epoch 49, batch   510] loss: 0.002
[epoch 49, batch   540] loss: 0.000
[epoch 49, batch   570] loss: 0.002
[epoch 49, batch   600] loss: 0.001
[epoch 49, batch   630] loss: 0.001
[epoch 49, batch   660] loss: 0.002
[epoch 49, batch   690] loss: 0.003
epoch 49 mean loss: 0.0049
[epoch 50, batch    30] loss: 0.000
[epoch 50, batch    60] loss: 0.002
[epoch 50, batch    90] loss: 0.000
[epoch 50, batch   120] loss: 0.007
[epoch 50, batch   150] loss: 0.007
[epoch 50, batch   180] loss: 0.000
[epoch 50, batch   210] loss: 0.007
[epoch 50, batch   240] loss: 0.005
[epoch 50, batch   270] loss: 0.004
[epoch 50, batch   300] loss: 0.001
[epoch 50, batch   330] loss: 0.005
[epoch 50, batch   360] loss: 0.005
[epoch 50, batch   390] loss: 0.000
[epoch 50, batch   420] loss: 0.012
[epoch 50, batch   450] loss: 0.000
[epoch 50, batch   480] loss: 0.002
[epoch 50, batch   510] loss: 0.000
[epoch 50, batch   540] loss: 0.001
[epoch 50, batch   570] loss: 0.000
[epoch 50, batch   600] loss: 0.001
[epoch 50, batch   630] loss: 0.004
[epoch 50, batch   660] loss: 0.001
[epoch 50, batch   690] loss: 0.014
epoch 50 mean loss: 0.0034
Training Completed.
Debiasing for 10 epochs.
[epoch 1, batch    30] loss: 0.000
Traceback (most recent call last):
  File "/home/zl22853/code/GeoBS/baselines/main.py", line 319, in <module>
    main()
  File "/home/zl22853/code/GeoBS/baselines/main.py", line 171, in main
    train_debias(task = task,
  File "/home/zl22853/code/GeoBS/baselines/TorchSpatial/modules/trainer.py", line 138, in train_debias
    gbs_loss = torch.mean(torch.stack(gbs_losses))
RuntimeError: stack expects a non-empty TensorList
