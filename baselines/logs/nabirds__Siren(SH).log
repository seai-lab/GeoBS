/home/zl22853/code/GeoBS/baselines/main.py:150: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)
  lats, lons = np.radians(loc_tr[:,1]), np.radians(loc_tr[:,0])
GPU Name: NVIDIA RTX A6000
Loading nabirds_with_loc_2019.json - train
   using meta data: ebird_meta
	 22819 total entries
	 22819 entries with images
	 22599 entries with meta data
Loading nabirds_with_loc_2019.json - test
   using meta data: ebird_meta
	 24633 total entries
	 24633 entries with images
	 24411 entries with meta data
Check the radian of input data! tensor([-117,   33])
Training for 50 epochs.
[epoch 1, batch    30] loss: 6.304
[epoch 1, batch    60] loss: 6.173
[epoch 1, batch    90] loss: 5.567
[epoch 1, batch   120] loss: 4.444
[epoch 1, batch   150] loss: 3.212
[epoch 1, batch   180] loss: 2.215
[epoch 1, batch   210] loss: 1.651
[epoch 1, batch   240] loss: 1.287
[epoch 1, batch   270] loss: 1.256
[epoch 1, batch   300] loss: 0.921
[epoch 1, batch   330] loss: 0.881
[epoch 1, batch   360] loss: 1.038
[epoch 1, batch   390] loss: 0.831
[epoch 1, batch   420] loss: 0.778
[epoch 1, batch   450] loss: 0.752
[epoch 1, batch   480] loss: 0.753
[epoch 1, batch   510] loss: 0.598
[epoch 1, batch   540] loss: 0.801
[epoch 1, batch   570] loss: 0.664
[epoch 1, batch   600] loss: 0.664
[epoch 1, batch   630] loss: 0.634
[epoch 1, batch   660] loss: 0.602
[epoch 1, batch   690] loss: 0.515
epoch 1 mean loss: 1.8203
[epoch 2, batch    30] loss: 0.456
[epoch 2, batch    60] loss: 0.474
[epoch 2, batch    90] loss: 0.435
[epoch 2, batch   120] loss: 0.398
[epoch 2, batch   150] loss: 0.445
[epoch 2, batch   180] loss: 0.447
[epoch 2, batch   210] loss: 0.329
[epoch 2, batch   240] loss: 0.469
[epoch 2, batch   270] loss: 0.448
[epoch 2, batch   300] loss: 0.362
[epoch 2, batch   330] loss: 0.470
[epoch 2, batch   360] loss: 0.448
[epoch 2, batch   390] loss: 0.406
[epoch 2, batch   420] loss: 0.437
[epoch 2, batch   450] loss: 0.388
[epoch 2, batch   480] loss: 0.422
[epoch 2, batch   510] loss: 0.443
[epoch 2, batch   540] loss: 0.394
[epoch 2, batch   570] loss: 0.468
[epoch 2, batch   600] loss: 0.401
[epoch 2, batch   630] loss: 0.327
[epoch 2, batch   660] loss: 0.343
[epoch 2, batch   690] loss: 0.431
epoch 2 mean loss: 0.4173
[epoch 3, batch    30] loss: 0.361
[epoch 3, batch    60] loss: 0.346
[epoch 3, batch    90] loss: 0.332
[epoch 3, batch   120] loss: 0.235
[epoch 3, batch   150] loss: 0.346
[epoch 3, batch   180] loss: 0.316
[epoch 3, batch   210] loss: 0.338
[epoch 3, batch   240] loss: 0.380
[epoch 3, batch   270] loss: 0.309
[epoch 3, batch   300] loss: 0.305
[epoch 3, batch   330] loss: 0.287
[epoch 3, batch   360] loss: 0.339
[epoch 3, batch   390] loss: 0.339
[epoch 3, batch   420] loss: 0.329
[epoch 3, batch   450] loss: 0.386
[epoch 3, batch   480] loss: 0.281
[epoch 3, batch   510] loss: 0.325
[epoch 3, batch   540] loss: 0.300
[epoch 3, batch   570] loss: 0.295
[epoch 3, batch   600] loss: 0.312
[epoch 3, batch   630] loss: 0.302
[epoch 3, batch   660] loss: 0.339
[epoch 3, batch   690] loss: 0.310
epoch 3 mean loss: 0.3219
[epoch 4, batch    30] loss: 0.283
[epoch 4, batch    60] loss: 0.261
[epoch 4, batch    90] loss: 0.206
[epoch 4, batch   120] loss: 0.310
[epoch 4, batch   150] loss: 0.229
[epoch 4, batch   180] loss: 0.223
[epoch 4, batch   210] loss: 0.225
[epoch 4, batch   240] loss: 0.302
[epoch 4, batch   270] loss: 0.236
[epoch 4, batch   300] loss: 0.260
[epoch 4, batch   330] loss: 0.235
[epoch 4, batch   360] loss: 0.241
[epoch 4, batch   390] loss: 0.247
[epoch 4, batch   420] loss: 0.235
[epoch 4, batch   450] loss: 0.222
[epoch 4, batch   480] loss: 0.213
[epoch 4, batch   510] loss: 0.296
[epoch 4, batch   540] loss: 0.256
[epoch 4, batch   570] loss: 0.298
[epoch 4, batch   600] loss: 0.260
[epoch 4, batch   630] loss: 0.274
[epoch 4, batch   660] loss: 0.224
[epoch 4, batch   690] loss: 0.291
epoch 4 mean loss: 0.2528
[epoch 5, batch    30] loss: 0.190
[epoch 5, batch    60] loss: 0.226
[epoch 5, batch    90] loss: 0.191
[epoch 5, batch   120] loss: 0.262
[epoch 5, batch   150] loss: 0.148
[epoch 5, batch   180] loss: 0.199
[epoch 5, batch   210] loss: 0.172
[epoch 5, batch   240] loss: 0.161
[epoch 5, batch   270] loss: 0.187
[epoch 5, batch   300] loss: 0.246
[epoch 5, batch   330] loss: 0.206
[epoch 5, batch   360] loss: 0.214
[epoch 5, batch   390] loss: 0.155
[epoch 5, batch   420] loss: 0.235
[epoch 5, batch   450] loss: 0.276
[epoch 5, batch   480] loss: 0.158
[epoch 5, batch   510] loss: 0.221
[epoch 5, batch   540] loss: 0.228
[epoch 5, batch   570] loss: 0.192
[epoch 5, batch   600] loss: 0.212
[epoch 5, batch   630] loss: 0.158
[epoch 5, batch   660] loss: 0.215
[epoch 5, batch   690] loss: 0.229
epoch 5 mean loss: 0.2051
[epoch 6, batch    30] loss: 0.205
[epoch 6, batch    60] loss: 0.153
[epoch 6, batch    90] loss: 0.212
[epoch 6, batch   120] loss: 0.207
[epoch 6, batch   150] loss: 0.163
[epoch 6, batch   180] loss: 0.148
[epoch 6, batch   210] loss: 0.170
[epoch 6, batch   240] loss: 0.171
[epoch 6, batch   270] loss: 0.138
[epoch 6, batch   300] loss: 0.155
[epoch 6, batch   330] loss: 0.180
[epoch 6, batch   360] loss: 0.177
[epoch 6, batch   390] loss: 0.185
[epoch 6, batch   420] loss: 0.159
[epoch 6, batch   450] loss: 0.145
[epoch 6, batch   480] loss: 0.124
[epoch 6, batch   510] loss: 0.155
[epoch 6, batch   540] loss: 0.162
[epoch 6, batch   570] loss: 0.155
[epoch 6, batch   600] loss: 0.220
[epoch 6, batch   630] loss: 0.222
[epoch 6, batch   660] loss: 0.217
[epoch 6, batch   690] loss: 0.166
epoch 6 mean loss: 0.1736
[epoch 7, batch    30] loss: 0.174
[epoch 7, batch    60] loss: 0.171
[epoch 7, batch    90] loss: 0.132
[epoch 7, batch   120] loss: 0.142
[epoch 7, batch   150] loss: 0.185
[epoch 7, batch   180] loss: 0.139
[epoch 7, batch   210] loss: 0.143
[epoch 7, batch   240] loss: 0.147
[epoch 7, batch   270] loss: 0.182
[epoch 7, batch   300] loss: 0.123
[epoch 7, batch   330] loss: 0.117
[epoch 7, batch   360] loss: 0.145
[epoch 7, batch   390] loss: 0.132
[epoch 7, batch   420] loss: 0.170
[epoch 7, batch   450] loss: 0.078
[epoch 7, batch   480] loss: 0.154
[epoch 7, batch   510] loss: 0.131
[epoch 7, batch   540] loss: 0.177
[epoch 7, batch   570] loss: 0.191
[epoch 7, batch   600] loss: 0.152
[epoch 7, batch   630] loss: 0.154
[epoch 7, batch   660] loss: 0.119
[epoch 7, batch   690] loss: 0.130
epoch 7 mean loss: 0.1475
[epoch 8, batch    30] loss: 0.126
[epoch 8, batch    60] loss: 0.103
[epoch 8, batch    90] loss: 0.128
[epoch 8, batch   120] loss: 0.143
[epoch 8, batch   150] loss: 0.097
[epoch 8, batch   180] loss: 0.080
[epoch 8, batch   210] loss: 0.143
[epoch 8, batch   240] loss: 0.137
[epoch 8, batch   270] loss: 0.126
[epoch 8, batch   300] loss: 0.146
[epoch 8, batch   330] loss: 0.108
[epoch 8, batch   360] loss: 0.128
[epoch 8, batch   390] loss: 0.104
[epoch 8, batch   420] loss: 0.138
[epoch 8, batch   450] loss: 0.098
[epoch 8, batch   480] loss: 0.105
[epoch 8, batch   510] loss: 0.141
[epoch 8, batch   540] loss: 0.151
[epoch 8, batch   570] loss: 0.118
[epoch 8, batch   600] loss: 0.143
[epoch 8, batch   630] loss: 0.122
[epoch 8, batch   660] loss: 0.155
[epoch 8, batch   690] loss: 0.120
epoch 8 mean loss: 0.1246
[epoch 9, batch    30] loss: 0.139
[epoch 9, batch    60] loss: 0.078
[epoch 9, batch    90] loss: 0.099
[epoch 9, batch   120] loss: 0.131
[epoch 9, batch   150] loss: 0.107
[epoch 9, batch   180] loss: 0.145
[epoch 9, batch   210] loss: 0.106
[epoch 9, batch   240] loss: 0.085
[epoch 9, batch   270] loss: 0.116
[epoch 9, batch   300] loss: 0.136
[epoch 9, batch   330] loss: 0.125
[epoch 9, batch   360] loss: 0.107
[epoch 9, batch   390] loss: 0.153
[epoch 9, batch   420] loss: 0.119
[epoch 9, batch   450] loss: 0.092
[epoch 9, batch   480] loss: 0.117
[epoch 9, batch   510] loss: 0.177
[epoch 9, batch   540] loss: 0.091
[epoch 9, batch   570] loss: 0.121
[epoch 9, batch   600] loss: 0.146
[epoch 9, batch   630] loss: 0.169
[epoch 9, batch   660] loss: 0.134
[epoch 9, batch   690] loss: 0.080
epoch 9 mean loss: 0.1207
[epoch 10, batch    30] loss: 0.083
[epoch 10, batch    60] loss: 0.087
[epoch 10, batch    90] loss: 0.119
[epoch 10, batch   120] loss: 0.086
[epoch 10, batch   150] loss: 0.125
[epoch 10, batch   180] loss: 0.102
[epoch 10, batch   210] loss: 0.121
[epoch 10, batch   240] loss: 0.096
[epoch 10, batch   270] loss: 0.084
[epoch 10, batch   300] loss: 0.140
[epoch 10, batch   330] loss: 0.107
[epoch 10, batch   360] loss: 0.117
[epoch 10, batch   390] loss: 0.124
[epoch 10, batch   420] loss: 0.117
[epoch 10, batch   450] loss: 0.174
[epoch 10, batch   480] loss: 0.141
[epoch 10, batch   510] loss: 0.103
[epoch 10, batch   540] loss: 0.094
[epoch 10, batch   570] loss: 0.126
[epoch 10, batch   600] loss: 0.088
[epoch 10, batch   630] loss: 0.075
[epoch 10, batch   660] loss: 0.075
[epoch 10, batch   690] loss: 0.108
epoch 10 mean loss: 0.1072
[epoch 11, batch    30] loss: 0.119
[epoch 11, batch    60] loss: 0.095
[epoch 11, batch    90] loss: 0.145
[epoch 11, batch   120] loss: 0.092
[epoch 11, batch   150] loss: 0.112
[epoch 11, batch   180] loss: 0.094
[epoch 11, batch   210] loss: 0.120
[epoch 11, batch   240] loss: 0.098
[epoch 11, batch   270] loss: 0.074
[epoch 11, batch   300] loss: 0.124
[epoch 11, batch   330] loss: 0.100
[epoch 11, batch   360] loss: 0.050
[epoch 11, batch   390] loss: 0.091
[epoch 11, batch   420] loss: 0.102
[epoch 11, batch   450] loss: 0.100
[epoch 11, batch   480] loss: 0.087
[epoch 11, batch   510] loss: 0.070
[epoch 11, batch   540] loss: 0.110
[epoch 11, batch   570] loss: 0.120
[epoch 11, batch   600] loss: 0.079
[epoch 11, batch   630] loss: 0.109
[epoch 11, batch   660] loss: 0.088
[epoch 11, batch   690] loss: 0.082
epoch 11 mean loss: 0.0975
[epoch 12, batch    30] loss: 0.090
[epoch 12, batch    60] loss: 0.050
[epoch 12, batch    90] loss: 0.056
[epoch 12, batch   120] loss: 0.071
[epoch 12, batch   150] loss: 0.098
[epoch 12, batch   180] loss: 0.090
[epoch 12, batch   210] loss: 0.126
[epoch 12, batch   240] loss: 0.109
[epoch 12, batch   270] loss: 0.058
[epoch 12, batch   300] loss: 0.048
[epoch 12, batch   330] loss: 0.094
[epoch 12, batch   360] loss: 0.096
[epoch 12, batch   390] loss: 0.101
[epoch 12, batch   420] loss: 0.099
[epoch 12, batch   450] loss: 0.098
[epoch 12, batch   480] loss: 0.078
[epoch 12, batch   510] loss: 0.140
[epoch 12, batch   540] loss: 0.081
[epoch 12, batch   570] loss: 0.067
[epoch 12, batch   600] loss: 0.074
[epoch 12, batch   630] loss: 0.125
[epoch 12, batch   660] loss: 0.079
[epoch 12, batch   690] loss: 0.076
epoch 12 mean loss: 0.0871
[epoch 13, batch    30] loss: 0.079
[epoch 13, batch    60] loss: 0.088
[epoch 13, batch    90] loss: 0.082
[epoch 13, batch   120] loss: 0.075
[epoch 13, batch   150] loss: 0.117
[epoch 13, batch   180] loss: 0.104
[epoch 13, batch   210] loss: 0.097
[epoch 13, batch   240] loss: 0.080
[epoch 13, batch   270] loss: 0.091
[epoch 13, batch   300] loss: 0.079
[epoch 13, batch   330] loss: 0.093
[epoch 13, batch   360] loss: 0.083
[epoch 13, batch   390] loss: 0.064
[epoch 13, batch   420] loss: 0.086
[epoch 13, batch   450] loss: 0.086
[epoch 13, batch   480] loss: 0.058
[epoch 13, batch   510] loss: 0.093
[epoch 13, batch   540] loss: 0.053
[epoch 13, batch   570] loss: 0.111
[epoch 13, batch   600] loss: 0.118
[epoch 13, batch   630] loss: 0.163
[epoch 13, batch   660] loss: 0.096
[epoch 13, batch   690] loss: 0.114
epoch 13 mean loss: 0.0917
[epoch 14, batch    30] loss: 0.070
[epoch 14, batch    60] loss: 0.041
[epoch 14, batch    90] loss: 0.088
[epoch 14, batch   120] loss: 0.072
[epoch 14, batch   150] loss: 0.060
[epoch 14, batch   180] loss: 0.057
[epoch 14, batch   210] loss: 0.089
[epoch 14, batch   240] loss: 0.051
[epoch 14, batch   270] loss: 0.061
[epoch 14, batch   300] loss: 0.060
[epoch 14, batch   330] loss: 0.096
[epoch 14, batch   360] loss: 0.104
[epoch 14, batch   390] loss: 0.077
[epoch 14, batch   420] loss: 0.075
[epoch 14, batch   450] loss: 0.077
[epoch 14, batch   480] loss: 0.104
[epoch 14, batch   510] loss: 0.058
[epoch 14, batch   540] loss: 0.086
[epoch 14, batch   570] loss: 0.113
[epoch 14, batch   600] loss: 0.078
[epoch 14, batch   630] loss: 0.073
[epoch 14, batch   660] loss: 0.126
[epoch 14, batch   690] loss: 0.062
epoch 14 mean loss: 0.0765
[epoch 15, batch    30] loss: 0.071
[epoch 15, batch    60] loss: 0.094
[epoch 15, batch    90] loss: 0.119
[epoch 15, batch   120] loss: 0.057
[epoch 15, batch   150] loss: 0.068
[epoch 15, batch   180] loss: 0.072
[epoch 15, batch   210] loss: 0.082
[epoch 15, batch   240] loss: 0.063
[epoch 15, batch   270] loss: 0.062
[epoch 15, batch   300] loss: 0.079
[epoch 15, batch   330] loss: 0.077
[epoch 15, batch   360] loss: 0.088
[epoch 15, batch   390] loss: 0.109
[epoch 15, batch   420] loss: 0.069
[epoch 15, batch   450] loss: 0.058
[epoch 15, batch   480] loss: 0.085
[epoch 15, batch   510] loss: 0.051
[epoch 15, batch   540] loss: 0.060
[epoch 15, batch   570] loss: 0.053
[epoch 15, batch   600] loss: 0.070
[epoch 15, batch   630] loss: 0.122
[epoch 15, batch   660] loss: 0.084
[epoch 15, batch   690] loss: 0.083
epoch 15 mean loss: 0.0779
[epoch 16, batch    30] loss: 0.083
[epoch 16, batch    60] loss: 0.083
[epoch 16, batch    90] loss: 0.084
[epoch 16, batch   120] loss: 0.065
[epoch 16, batch   150] loss: 0.083
[epoch 16, batch   180] loss: 0.053
[epoch 16, batch   210] loss: 0.071
[epoch 16, batch   240] loss: 0.105
[epoch 16, batch   270] loss: 0.081
[epoch 16, batch   300] loss: 0.079
[epoch 16, batch   330] loss: 0.058
[epoch 16, batch   360] loss: 0.050
[epoch 16, batch   390] loss: 0.092
[epoch 16, batch   420] loss: 0.120
[epoch 16, batch   450] loss: 0.058
[epoch 16, batch   480] loss: 0.108
[epoch 16, batch   510] loss: 0.067
[epoch 16, batch   540] loss: 0.087
[epoch 16, batch   570] loss: 0.081
[epoch 16, batch   600] loss: 0.062
[epoch 16, batch   630] loss: 0.059
[epoch 16, batch   660] loss: 0.119
[epoch 16, batch   690] loss: 0.059
epoch 16 mean loss: 0.0784
[epoch 17, batch    30] loss: 0.092
[epoch 17, batch    60] loss: 0.045
[epoch 17, batch    90] loss: 0.074
[epoch 17, batch   120] loss: 0.044
[epoch 17, batch   150] loss: 0.041
[epoch 17, batch   180] loss: 0.039
[epoch 17, batch   210] loss: 0.049
[epoch 17, batch   240] loss: 0.056
[epoch 17, batch   270] loss: 0.118
[epoch 17, batch   300] loss: 0.101
[epoch 17, batch   330] loss: 0.058
[epoch 17, batch   360] loss: 0.058
[epoch 17, batch   390] loss: 0.044
[epoch 17, batch   420] loss: 0.046
[epoch 17, batch   450] loss: 0.047
[epoch 17, batch   480] loss: 0.052
[epoch 17, batch   510] loss: 0.046
[epoch 17, batch   540] loss: 0.102
[epoch 17, batch   570] loss: 0.055
[epoch 17, batch   600] loss: 0.065
[epoch 17, batch   630] loss: 0.094
[epoch 17, batch   660] loss: 0.099
[epoch 17, batch   690] loss: 0.065
epoch 17 mean loss: 0.0647
[epoch 18, batch    30] loss: 0.084
[epoch 18, batch    60] loss: 0.041
[epoch 18, batch    90] loss: 0.053
[epoch 18, batch   120] loss: 0.072
[epoch 18, batch   150] loss: 0.048
[epoch 18, batch   180] loss: 0.060
[epoch 18, batch   210] loss: 0.067
[epoch 18, batch   240] loss: 0.061
[epoch 18, batch   270] loss: 0.069
[epoch 18, batch   300] loss: 0.040
[epoch 18, batch   330] loss: 0.061
[epoch 18, batch   360] loss: 0.055
[epoch 18, batch   390] loss: 0.046
[epoch 18, batch   420] loss: 0.062
[epoch 18, batch   450] loss: 0.089
[epoch 18, batch   480] loss: 0.028
[epoch 18, batch   510] loss: 0.096
[epoch 18, batch   540] loss: 0.065
[epoch 18, batch   570] loss: 0.092
[epoch 18, batch   600] loss: 0.116
[epoch 18, batch   630] loss: 0.069
[epoch 18, batch   660] loss: 0.096
[epoch 18, batch   690] loss: 0.067
epoch 18 mean loss: 0.0669
[epoch 19, batch    30] loss: 0.095
[epoch 19, batch    60] loss: 0.062
[epoch 19, batch    90] loss: 0.070
[epoch 19, batch   120] loss: 0.077
[epoch 19, batch   150] loss: 0.018
[epoch 19, batch   180] loss: 0.101
[epoch 19, batch   210] loss: 0.095
[epoch 19, batch   240] loss: 0.073
[epoch 19, batch   270] loss: 0.088
[epoch 19, batch   300] loss: 0.067
[epoch 19, batch   330] loss: 0.072
[epoch 19, batch   360] loss: 0.080
[epoch 19, batch   390] loss: 0.056
[epoch 19, batch   420] loss: 0.079
[epoch 19, batch   450] loss: 0.083
[epoch 19, batch   480] loss: 0.077
[epoch 19, batch   510] loss: 0.098
[epoch 19, batch   540] loss: 0.101
[epoch 19, batch   570] loss: 0.036
[epoch 19, batch   600] loss: 0.100
[epoch 19, batch   630] loss: 0.054
[epoch 19, batch   660] loss: 0.072
[epoch 19, batch   690] loss: 0.093
epoch 19 mean loss: 0.0754
[epoch 20, batch    30] loss: 0.039
[epoch 20, batch    60] loss: 0.080
[epoch 20, batch    90] loss: 0.067
[epoch 20, batch   120] loss: 0.084
[epoch 20, batch   150] loss: 0.088
[epoch 20, batch   180] loss: 0.077
[epoch 20, batch   210] loss: 0.040
[epoch 20, batch   240] loss: 0.053
[epoch 20, batch   270] loss: 0.051
[epoch 20, batch   300] loss: 0.056
[epoch 20, batch   330] loss: 0.079
[epoch 20, batch   360] loss: 0.085
[epoch 20, batch   390] loss: 0.079
[epoch 20, batch   420] loss: 0.057
[epoch 20, batch   450] loss: 0.082
[epoch 20, batch   480] loss: 0.086
[epoch 20, batch   510] loss: 0.032
[epoch 20, batch   540] loss: 0.114
[epoch 20, batch   570] loss: 0.095
[epoch 20, batch   600] loss: 0.066
[epoch 20, batch   630] loss: 0.035
[epoch 20, batch   660] loss: 0.066
[epoch 20, batch   690] loss: 0.053
epoch 20 mean loss: 0.0679
[epoch 21, batch    30] loss: 0.088
[epoch 21, batch    60] loss: 0.057
[epoch 21, batch    90] loss: 0.050
[epoch 21, batch   120] loss: 0.073
[epoch 21, batch   150] loss: 0.020
[epoch 21, batch   180] loss: 0.064
[epoch 21, batch   210] loss: 0.034
[epoch 21, batch   240] loss: 0.045
[epoch 21, batch   270] loss: 0.043
[epoch 21, batch   300] loss: 0.030
[epoch 21, batch   330] loss: 0.020
[epoch 21, batch   360] loss: 0.035
[epoch 21, batch   390] loss: 0.019
[epoch 21, batch   420] loss: 0.040
[epoch 21, batch   450] loss: 0.045
[epoch 21, batch   480] loss: 0.032
[epoch 21, batch   510] loss: 0.022
[epoch 21, batch   540] loss: 0.018
[epoch 21, batch   570] loss: 0.026
[epoch 21, batch   600] loss: 0.017
[epoch 21, batch   630] loss: 0.050
[epoch 21, batch   660] loss: 0.025
[epoch 21, batch   690] loss: 0.046
epoch 21 mean loss: 0.0386
[epoch 22, batch    30] loss: 0.019
[epoch 22, batch    60] loss: 0.015
[epoch 22, batch    90] loss: 0.034
[epoch 22, batch   120] loss: 0.028
[epoch 22, batch   150] loss: 0.022
[epoch 22, batch   180] loss: 0.021
[epoch 22, batch   210] loss: 0.033
[epoch 22, batch   240] loss: 0.061
[epoch 22, batch   270] loss: 0.025
[epoch 22, batch   300] loss: 0.025
[epoch 22, batch   330] loss: 0.018
[epoch 22, batch   360] loss: 0.019
[epoch 22, batch   390] loss: 0.039
[epoch 22, batch   420] loss: 0.048
[epoch 22, batch   450] loss: 0.023
[epoch 22, batch   480] loss: 0.016
[epoch 22, batch   510] loss: 0.043
[epoch 22, batch   540] loss: 0.041
[epoch 22, batch   570] loss: 0.013
[epoch 22, batch   600] loss: 0.017
[epoch 22, batch   630] loss: 0.018
[epoch 22, batch   660] loss: 0.046
[epoch 22, batch   690] loss: 0.024
epoch 22 mean loss: 0.0285
[epoch 23, batch    30] loss: 0.026
[epoch 23, batch    60] loss: 0.028
[epoch 23, batch    90] loss: 0.010
[epoch 23, batch   120] loss: 0.014
[epoch 23, batch   150] loss: 0.032
[epoch 23, batch   180] loss: 0.013
[epoch 23, batch   210] loss: 0.016
[epoch 23, batch   240] loss: 0.013
[epoch 23, batch   270] loss: 0.021
[epoch 23, batch   300] loss: 0.057
[epoch 23, batch   330] loss: 0.013
[epoch 23, batch   360] loss: 0.018
[epoch 23, batch   390] loss: 0.029
[epoch 23, batch   420] loss: 0.008
[epoch 23, batch   450] loss: 0.008
[epoch 23, batch   480] loss: 0.005
[epoch 23, batch   510] loss: 0.015
[epoch 23, batch   540] loss: 0.006
[epoch 23, batch   570] loss: 0.020
[epoch 23, batch   600] loss: 0.005
[epoch 23, batch   630] loss: 0.033
[epoch 23, batch   660] loss: 0.018
[epoch 23, batch   690] loss: 0.015
epoch 23 mean loss: 0.0183
[epoch 24, batch    30] loss: 0.015
[epoch 24, batch    60] loss: 0.009
[epoch 24, batch    90] loss: 0.024
[epoch 24, batch   120] loss: 0.012
[epoch 24, batch   150] loss: 0.034
[epoch 24, batch   180] loss: 0.034
[epoch 24, batch   210] loss: 0.038
[epoch 24, batch   240] loss: 0.020
[epoch 24, batch   270] loss: 0.014
[epoch 24, batch   300] loss: 0.039
[epoch 24, batch   330] loss: 0.026
[epoch 24, batch   360] loss: 0.012
[epoch 24, batch   390] loss: 0.025
[epoch 24, batch   420] loss: 0.030
[epoch 24, batch   450] loss: 0.017
[epoch 24, batch   480] loss: 0.047
[epoch 24, batch   510] loss: 0.050
[epoch 24, batch   540] loss: 0.046
[epoch 24, batch   570] loss: 0.058
[epoch 24, batch   600] loss: 0.046
[epoch 24, batch   630] loss: 0.037
[epoch 24, batch   660] loss: 0.017
[epoch 24, batch   690] loss: 0.044
epoch 24 mean loss: 0.0303
[epoch 25, batch    30] loss: 0.027
[epoch 25, batch    60] loss: 0.028
[epoch 25, batch    90] loss: 0.012
[epoch 25, batch   120] loss: 0.019
[epoch 25, batch   150] loss: 0.039
[epoch 25, batch   180] loss: 0.011
[epoch 25, batch   210] loss: 0.057
[epoch 25, batch   240] loss: 0.017
[epoch 25, batch   270] loss: 0.006
[epoch 25, batch   300] loss: 0.009
[epoch 25, batch   330] loss: 0.009
[epoch 25, batch   360] loss: 0.020
[epoch 25, batch   390] loss: 0.014
[epoch 25, batch   420] loss: 0.062
[epoch 25, batch   450] loss: 0.044
[epoch 25, batch   480] loss: 0.018
[epoch 25, batch   510] loss: 0.020
[epoch 25, batch   540] loss: 0.013
[epoch 25, batch   570] loss: 0.023
[epoch 25, batch   600] loss: 0.016
[epoch 25, batch   630] loss: 0.029
[epoch 25, batch   660] loss: 0.032
[epoch 25, batch   690] loss: 0.018
epoch 25 mean loss: 0.0236
[epoch 26, batch    30] loss: 0.013
[epoch 26, batch    60] loss: 0.018
[epoch 26, batch    90] loss: 0.018
[epoch 26, batch   120] loss: 0.023
[epoch 26, batch   150] loss: 0.025
[epoch 26, batch   180] loss: 0.037
[epoch 26, batch   210] loss: 0.007
[epoch 26, batch   240] loss: 0.039
[epoch 26, batch   270] loss: 0.036
[epoch 26, batch   300] loss: 0.015
[epoch 26, batch   330] loss: 0.016
[epoch 26, batch   360] loss: 0.021
[epoch 26, batch   390] loss: 0.035
[epoch 26, batch   420] loss: 0.012
[epoch 26, batch   450] loss: 0.021
[epoch 26, batch   480] loss: 0.019
[epoch 26, batch   510] loss: 0.040
[epoch 26, batch   540] loss: 0.030
[epoch 26, batch   570] loss: 0.019
[epoch 26, batch   600] loss: 0.032
[epoch 26, batch   630] loss: 0.015
[epoch 26, batch   660] loss: 0.009
[epoch 26, batch   690] loss: 0.004
epoch 26 mean loss: 0.0216
[epoch 27, batch    30] loss: 0.014
[epoch 27, batch    60] loss: 0.014
[epoch 27, batch    90] loss: 0.014
[epoch 27, batch   120] loss: 0.016
[epoch 27, batch   150] loss: 0.011
[epoch 27, batch   180] loss: 0.021
[epoch 27, batch   210] loss: 0.039
[epoch 27, batch   240] loss: 0.020
[epoch 27, batch   270] loss: 0.007
[epoch 27, batch   300] loss: 0.016
[epoch 27, batch   330] loss: 0.021
[epoch 27, batch   360] loss: 0.002
[epoch 27, batch   390] loss: 0.007
[epoch 27, batch   420] loss: 0.008
[epoch 27, batch   450] loss: 0.008
[epoch 27, batch   480] loss: 0.016
[epoch 27, batch   510] loss: 0.010
[epoch 27, batch   540] loss: 0.022
[epoch 27, batch   570] loss: 0.021
[epoch 27, batch   600] loss: 0.008
[epoch 27, batch   630] loss: 0.012
[epoch 27, batch   660] loss: 0.005
[epoch 27, batch   690] loss: 0.027
epoch 27 mean loss: 0.0150
[epoch 28, batch    30] loss: 0.025
[epoch 28, batch    60] loss: 0.018
[epoch 28, batch    90] loss: 0.011
[epoch 28, batch   120] loss: 0.005
[epoch 28, batch   150] loss: 0.002
[epoch 28, batch   180] loss: 0.007
[epoch 28, batch   210] loss: 0.028
[epoch 28, batch   240] loss: 0.026
[epoch 28, batch   270] loss: 0.006
[epoch 28, batch   300] loss: 0.008
[epoch 28, batch   330] loss: 0.007
[epoch 28, batch   360] loss: 0.002
[epoch 28, batch   390] loss: 0.012
[epoch 28, batch   420] loss: 0.005
[epoch 28, batch   450] loss: 0.025
[epoch 28, batch   480] loss: 0.008
[epoch 28, batch   510] loss: 0.011
[epoch 28, batch   540] loss: 0.003
[epoch 28, batch   570] loss: 0.006
[epoch 28, batch   600] loss: 0.016
[epoch 28, batch   630] loss: 0.020
[epoch 28, batch   660] loss: 0.008
[epoch 28, batch   690] loss: 0.011
epoch 28 mean loss: 0.0123
[epoch 29, batch    30] loss: 0.024
[epoch 29, batch    60] loss: 0.003
[epoch 29, batch    90] loss: 0.008
[epoch 29, batch   120] loss: 0.002
[epoch 29, batch   150] loss: 0.009
[epoch 29, batch   180] loss: 0.018
[epoch 29, batch   210] loss: 0.011
[epoch 29, batch   240] loss: 0.023
[epoch 29, batch   270] loss: 0.004
[epoch 29, batch   300] loss: 0.005
[epoch 29, batch   330] loss: 0.018
[epoch 29, batch   360] loss: 0.006
[epoch 29, batch   390] loss: 0.011
[epoch 29, batch   420] loss: 0.009
[epoch 29, batch   450] loss: 0.009
[epoch 29, batch   480] loss: 0.004
[epoch 29, batch   510] loss: 0.009
[epoch 29, batch   540] loss: 0.007
[epoch 29, batch   570] loss: 0.002
[epoch 29, batch   600] loss: 0.009
[epoch 29, batch   630] loss: 0.012
[epoch 29, batch   660] loss: 0.011
[epoch 29, batch   690] loss: 0.011
epoch 29 mean loss: 0.0095
[epoch 30, batch    30] loss: 0.007
[epoch 30, batch    60] loss: 0.012
[epoch 30, batch    90] loss: 0.008
[epoch 30, batch   120] loss: 0.008
[epoch 30, batch   150] loss: 0.005
[epoch 30, batch   180] loss: 0.015
[epoch 30, batch   210] loss: 0.004
[epoch 30, batch   240] loss: 0.003
[epoch 30, batch   270] loss: 0.002
[epoch 30, batch   300] loss: 0.002
[epoch 30, batch   330] loss: 0.007
[epoch 30, batch   360] loss: 0.014
[epoch 30, batch   390] loss: 0.024
[epoch 30, batch   420] loss: 0.004
[epoch 30, batch   450] loss: 0.011
[epoch 30, batch   480] loss: 0.007
[epoch 30, batch   510] loss: 0.003
[epoch 30, batch   540] loss: 0.016
[epoch 30, batch   570] loss: 0.003
[epoch 30, batch   600] loss: 0.004
[epoch 30, batch   630] loss: 0.023
[epoch 30, batch   660] loss: 0.008
[epoch 30, batch   690] loss: 0.011
epoch 30 mean loss: 0.0085
[epoch 31, batch    30] loss: 0.012
[epoch 31, batch    60] loss: 0.003
[epoch 31, batch    90] loss: 0.009
[epoch 31, batch   120] loss: 0.037
[epoch 31, batch   150] loss: 0.015
[epoch 31, batch   180] loss: 0.011
[epoch 31, batch   210] loss: 0.035
[epoch 31, batch   240] loss: 0.028
[epoch 31, batch   270] loss: 0.000
[epoch 31, batch   300] loss: 0.009
[epoch 31, batch   330] loss: 0.024
[epoch 31, batch   360] loss: 0.007
[epoch 31, batch   390] loss: 0.009
[epoch 31, batch   420] loss: 0.025
[epoch 31, batch   450] loss: 0.005
[epoch 31, batch   480] loss: 0.015
[epoch 31, batch   510] loss: 0.003
[epoch 31, batch   540] loss: 0.001
[epoch 31, batch   570] loss: 0.006
[epoch 31, batch   600] loss: 0.008
[epoch 31, batch   630] loss: 0.008
[epoch 31, batch   660] loss: 0.006
[epoch 31, batch   690] loss: 0.005
epoch 31 mean loss: 0.0121
[epoch 32, batch    30] loss: 0.013
[epoch 32, batch    60] loss: 0.014
[epoch 32, batch    90] loss: 0.010
[epoch 32, batch   120] loss: 0.009
[epoch 32, batch   150] loss: 0.013
[epoch 32, batch   180] loss: 0.008
[epoch 32, batch   210] loss: 0.027
[epoch 32, batch   240] loss: 0.004
[epoch 32, batch   270] loss: 0.022
[epoch 32, batch   300] loss: 0.018
[epoch 32, batch   330] loss: 0.004
[epoch 32, batch   360] loss: 0.004
[epoch 32, batch   390] loss: 0.008
[epoch 32, batch   420] loss: 0.012
[epoch 32, batch   450] loss: 0.009
[epoch 32, batch   480] loss: 0.011
[epoch 32, batch   510] loss: 0.005
[epoch 32, batch   540] loss: 0.016
[epoch 32, batch   570] loss: 0.016
[epoch 32, batch   600] loss: 0.005
[epoch 32, batch   630] loss: 0.009
[epoch 32, batch   660] loss: 0.008
[epoch 32, batch   690] loss: 0.021
epoch 32 mean loss: 0.0115
[epoch 33, batch    30] loss: 0.004
[epoch 33, batch    60] loss: 0.010
[epoch 33, batch    90] loss: 0.002
[epoch 33, batch   120] loss: 0.008
[epoch 33, batch   150] loss: 0.017
[epoch 33, batch   180] loss: 0.004
[epoch 33, batch   210] loss: 0.019
[epoch 33, batch   240] loss: 0.004
[epoch 33, batch   270] loss: 0.025
[epoch 33, batch   300] loss: 0.019
[epoch 33, batch   330] loss: 0.015
[epoch 33, batch   360] loss: 0.004
[epoch 33, batch   390] loss: 0.007
[epoch 33, batch   420] loss: 0.018
[epoch 33, batch   450] loss: 0.006
[epoch 33, batch   480] loss: 0.012
[epoch 33, batch   510] loss: 0.013
[epoch 33, batch   540] loss: 0.017
[epoch 33, batch   570] loss: 0.005
[epoch 33, batch   600] loss: 0.002
[epoch 33, batch   630] loss: 0.024
[epoch 33, batch   660] loss: 0.009
[epoch 33, batch   690] loss: 0.021
epoch 33 mean loss: 0.0114
[epoch 34, batch    30] loss: 0.005
[epoch 34, batch    60] loss: 0.012
[epoch 34, batch    90] loss: 0.013
[epoch 34, batch   120] loss: 0.005
[epoch 34, batch   150] loss: 0.003
[epoch 34, batch   180] loss: 0.001
[epoch 34, batch   210] loss: 0.009
[epoch 34, batch   240] loss: 0.009
[epoch 34, batch   270] loss: 0.005
[epoch 34, batch   300] loss: 0.011
[epoch 34, batch   330] loss: 0.027
[epoch 34, batch   360] loss: 0.007
[epoch 34, batch   390] loss: 0.006
[epoch 34, batch   420] loss: 0.005
[epoch 34, batch   450] loss: 0.012
[epoch 34, batch   480] loss: 0.012
[epoch 34, batch   510] loss: 0.012
[epoch 34, batch   540] loss: 0.007
[epoch 34, batch   570] loss: 0.006
[epoch 34, batch   600] loss: 0.004
[epoch 34, batch   630] loss: 0.011
[epoch 34, batch   660] loss: 0.008
[epoch 34, batch   690] loss: 0.013
epoch 34 mean loss: 0.0089
[epoch 35, batch    30] loss: 0.002
[epoch 35, batch    60] loss: 0.018
[epoch 35, batch    90] loss: 0.005
[epoch 35, batch   120] loss: 0.030
[epoch 35, batch   150] loss: 0.005
[epoch 35, batch   180] loss: 0.005
[epoch 35, batch   210] loss: 0.006
[epoch 35, batch   240] loss: 0.004
[epoch 35, batch   270] loss: 0.007
[epoch 35, batch   300] loss: 0.005
[epoch 35, batch   330] loss: 0.014
[epoch 35, batch   360] loss: 0.015
[epoch 35, batch   390] loss: 0.009
[epoch 35, batch   420] loss: 0.034
[epoch 35, batch   450] loss: 0.004
[epoch 35, batch   480] loss: 0.001
[epoch 35, batch   510] loss: 0.002
[epoch 35, batch   540] loss: 0.013
[epoch 35, batch   570] loss: 0.001
[epoch 35, batch   600] loss: 0.002
[epoch 35, batch   630] loss: 0.002
[epoch 35, batch   660] loss: 0.013
[epoch 35, batch   690] loss: 0.009
epoch 35 mean loss: 0.0089
[epoch 36, batch    30] loss: 0.009
[epoch 36, batch    60] loss: 0.001
[epoch 36, batch    90] loss: 0.001
[epoch 36, batch   120] loss: 0.003
[epoch 36, batch   150] loss: 0.002
[epoch 36, batch   180] loss: 0.009
[epoch 36, batch   210] loss: 0.009
[epoch 36, batch   240] loss: 0.001
[epoch 36, batch   270] loss: 0.004
[epoch 36, batch   300] loss: 0.002
[epoch 36, batch   330] loss: 0.015
[epoch 36, batch   360] loss: 0.002
[epoch 36, batch   390] loss: 0.006
[epoch 36, batch   420] loss: 0.000
[epoch 36, batch   450] loss: 0.014
[epoch 36, batch   480] loss: 0.002
[epoch 36, batch   510] loss: 0.010
[epoch 36, batch   540] loss: 0.009
[epoch 36, batch   570] loss: 0.012
[epoch 36, batch   600] loss: 0.002
[epoch 36, batch   630] loss: 0.003
[epoch 36, batch   660] loss: 0.006
[epoch 36, batch   690] loss: 0.008
epoch 36 mean loss: 0.0056
[epoch 37, batch    30] loss: 0.005
[epoch 37, batch    60] loss: 0.012
[epoch 37, batch    90] loss: 0.010
[epoch 37, batch   120] loss: 0.003
[epoch 37, batch   150] loss: 0.003
[epoch 37, batch   180] loss: 0.008
[epoch 37, batch   210] loss: 0.006
[epoch 37, batch   240] loss: 0.007
[epoch 37, batch   270] loss: 0.001
[epoch 37, batch   300] loss: 0.001
[epoch 37, batch   330] loss: 0.008
[epoch 37, batch   360] loss: 0.001
[epoch 37, batch   390] loss: 0.002
[epoch 37, batch   420] loss: 0.002
[epoch 37, batch   450] loss: 0.001
[epoch 37, batch   480] loss: 0.015
[epoch 37, batch   510] loss: 0.011
[epoch 37, batch   540] loss: 0.001
[epoch 37, batch   570] loss: 0.011
[epoch 37, batch   600] loss: 0.008
[epoch 37, batch   630] loss: 0.006
[epoch 37, batch   660] loss: 0.014
[epoch 37, batch   690] loss: 0.003
epoch 37 mean loss: 0.0059
[epoch 38, batch    30] loss: 0.006
[epoch 38, batch    60] loss: 0.001
[epoch 38, batch    90] loss: 0.001
[epoch 38, batch   120] loss: 0.001
[epoch 38, batch   150] loss: 0.003
[epoch 38, batch   180] loss: 0.001
[epoch 38, batch   210] loss: 0.000
[epoch 38, batch   240] loss: 0.001
[epoch 38, batch   270] loss: 0.001
[epoch 38, batch   300] loss: 0.018
[epoch 38, batch   330] loss: 0.012
[epoch 38, batch   360] loss: 0.005
[epoch 38, batch   390] loss: 0.001
[epoch 38, batch   420] loss: 0.007
[epoch 38, batch   450] loss: 0.007
[epoch 38, batch   480] loss: 0.004
[epoch 38, batch   510] loss: 0.006
[epoch 38, batch   540] loss: 0.004
[epoch 38, batch   570] loss: 0.005
[epoch 38, batch   600] loss: 0.009
[epoch 38, batch   630] loss: 0.002
[epoch 38, batch   660] loss: 0.000
[epoch 38, batch   690] loss: 0.018
epoch 38 mean loss: 0.0049
[epoch 39, batch    30] loss: 0.005
[epoch 39, batch    60] loss: 0.003
[epoch 39, batch    90] loss: 0.004
[epoch 39, batch   120] loss: 0.006
[epoch 39, batch   150] loss: 0.004
[epoch 39, batch   180] loss: 0.003
[epoch 39, batch   210] loss: 0.005
[epoch 39, batch   240] loss: 0.003
[epoch 39, batch   270] loss: 0.019
[epoch 39, batch   300] loss: 0.004
[epoch 39, batch   330] loss: 0.005
[epoch 39, batch   360] loss: 0.006
[epoch 39, batch   390] loss: 0.003
[epoch 39, batch   420] loss: 0.005
[epoch 39, batch   450] loss: 0.003
[epoch 39, batch   480] loss: 0.003
[epoch 39, batch   510] loss: 0.002
[epoch 39, batch   540] loss: 0.003
[epoch 39, batch   570] loss: 0.009
[epoch 39, batch   600] loss: 0.003
[epoch 39, batch   630] loss: 0.008
[epoch 39, batch   660] loss: 0.001
[epoch 39, batch   690] loss: 0.002
epoch 39 mean loss: 0.0047
[epoch 40, batch    30] loss: 0.021
[epoch 40, batch    60] loss: 0.002
[epoch 40, batch    90] loss: 0.002
[epoch 40, batch   120] loss: 0.001
[epoch 40, batch   150] loss: 0.012
[epoch 40, batch   180] loss: 0.001
[epoch 40, batch   210] loss: 0.018
[epoch 40, batch   240] loss: 0.006
[epoch 40, batch   270] loss: 0.015
[epoch 40, batch   300] loss: 0.000
[epoch 40, batch   330] loss: 0.001
[epoch 40, batch   360] loss: 0.006
[epoch 40, batch   390] loss: 0.002
[epoch 40, batch   420] loss: 0.009
[epoch 40, batch   450] loss: 0.000
[epoch 40, batch   480] loss: 0.005
[epoch 40, batch   510] loss: 0.007
[epoch 40, batch   540] loss: 0.018
[epoch 40, batch   570] loss: 0.002
[epoch 40, batch   600] loss: 0.002
[epoch 40, batch   630] loss: 0.015
[epoch 40, batch   660] loss: 0.003
[epoch 40, batch   690] loss: 0.016
epoch 40 mean loss: 0.0070
[epoch 41, batch    30] loss: 0.001
[epoch 41, batch    60] loss: 0.000
[epoch 41, batch    90] loss: 0.022
[epoch 41, batch   120] loss: 0.003
[epoch 41, batch   150] loss: 0.017
[epoch 41, batch   180] loss: 0.003
[epoch 41, batch   210] loss: 0.003
[epoch 41, batch   240] loss: 0.000
[epoch 41, batch   270] loss: 0.001
[epoch 41, batch   300] loss: 0.002
[epoch 41, batch   330] loss: 0.009
[epoch 41, batch   360] loss: 0.001
[epoch 41, batch   390] loss: 0.001
[epoch 41, batch   420] loss: 0.000
[epoch 41, batch   450] loss: 0.005
[epoch 41, batch   480] loss: 0.001
[epoch 41, batch   510] loss: 0.006
[epoch 41, batch   540] loss: 0.002
[epoch 41, batch   570] loss: 0.005
[epoch 41, batch   600] loss: 0.003
[epoch 41, batch   630] loss: 0.002
[epoch 41, batch   660] loss: 0.005
[epoch 41, batch   690] loss: 0.002
epoch 41 mean loss: 0.0041
[epoch 42, batch    30] loss: 0.002
[epoch 42, batch    60] loss: 0.010
[epoch 42, batch    90] loss: 0.003
[epoch 42, batch   120] loss: 0.003
[epoch 42, batch   150] loss: 0.001
[epoch 42, batch   180] loss: 0.003
[epoch 42, batch   210] loss: 0.008
[epoch 42, batch   240] loss: 0.018
[epoch 42, batch   270] loss: 0.018
[epoch 42, batch   300] loss: 0.000
[epoch 42, batch   330] loss: 0.001
[epoch 42, batch   360] loss: 0.004
[epoch 42, batch   390] loss: 0.004
[epoch 42, batch   420] loss: 0.003
[epoch 42, batch   450] loss: 0.001
[epoch 42, batch   480] loss: 0.004
[epoch 42, batch   510] loss: 0.011
[epoch 42, batch   540] loss: 0.013
[epoch 42, batch   570] loss: 0.002
[epoch 42, batch   600] loss: 0.012
[epoch 42, batch   630] loss: 0.007
[epoch 42, batch   660] loss: 0.001
[epoch 42, batch   690] loss: 0.003
epoch 42 mean loss: 0.0056
[epoch 43, batch    30] loss: 0.005
[epoch 43, batch    60] loss: 0.009
[epoch 43, batch    90] loss: 0.003
[epoch 43, batch   120] loss: 0.002
[epoch 43, batch   150] loss: 0.017
[epoch 43, batch   180] loss: 0.002
[epoch 43, batch   210] loss: 0.008
[epoch 43, batch   240] loss: 0.004
[epoch 43, batch   270] loss: 0.002
[epoch 43, batch   300] loss: 0.003
[epoch 43, batch   330] loss: 0.011
[epoch 43, batch   360] loss: 0.007
[epoch 43, batch   390] loss: 0.012
[epoch 43, batch   420] loss: 0.002
[epoch 43, batch   450] loss: 0.007
[epoch 43, batch   480] loss: 0.013
[epoch 43, batch   510] loss: 0.004
[epoch 43, batch   540] loss: 0.001
[epoch 43, batch   570] loss: 0.001
[epoch 43, batch   600] loss: 0.003
[epoch 43, batch   630] loss: 0.012
[epoch 43, batch   660] loss: 0.000
[epoch 43, batch   690] loss: 0.014
epoch 43 mean loss: 0.0059
[epoch 44, batch    30] loss: 0.000
[epoch 44, batch    60] loss: 0.001
[epoch 44, batch    90] loss: 0.005
[epoch 44, batch   120] loss: 0.001
[epoch 44, batch   150] loss: 0.002
[epoch 44, batch   180] loss: 0.001
[epoch 44, batch   210] loss: 0.006
[epoch 44, batch   240] loss: 0.001
[epoch 44, batch   270] loss: 0.013
[epoch 44, batch   300] loss: 0.003
[epoch 44, batch   330] loss: 0.012
[epoch 44, batch   360] loss: 0.020
[epoch 44, batch   390] loss: 0.002
[epoch 44, batch   420] loss: 0.003
[epoch 44, batch   450] loss: 0.001
[epoch 44, batch   480] loss: 0.003
[epoch 44, batch   510] loss: 0.001
[epoch 44, batch   540] loss: 0.007
[epoch 44, batch   570] loss: 0.001
[epoch 44, batch   600] loss: 0.003
[epoch 44, batch   630] loss: 0.003
[epoch 44, batch   660] loss: 0.003
[epoch 44, batch   690] loss: 0.004
epoch 44 mean loss: 0.0043
[epoch 45, batch    30] loss: 0.000
[epoch 45, batch    60] loss: 0.009
[epoch 45, batch    90] loss: 0.001
[epoch 45, batch   120] loss: 0.023
[epoch 45, batch   150] loss: 0.002
[epoch 45, batch   180] loss: 0.007
[epoch 45, batch   210] loss: 0.001
[epoch 45, batch   240] loss: 0.001
[epoch 45, batch   270] loss: 0.000
[epoch 45, batch   300] loss: 0.000
[epoch 45, batch   330] loss: 0.001
[epoch 45, batch   360] loss: 0.001
[epoch 45, batch   390] loss: 0.002
[epoch 45, batch   420] loss: 0.012
[epoch 45, batch   450] loss: 0.025
[epoch 45, batch   480] loss: 0.001
[epoch 45, batch   510] loss: 0.000
[epoch 45, batch   540] loss: 0.000
[epoch 45, batch   570] loss: 0.006
[epoch 45, batch   600] loss: 0.001
[epoch 45, batch   630] loss: 0.018
[epoch 45, batch   660] loss: 0.002
[epoch 45, batch   690] loss: 0.004
epoch 45 mean loss: 0.0052
[epoch 46, batch    30] loss: 0.001
[epoch 46, batch    60] loss: 0.005
[epoch 46, batch    90] loss: 0.002
[epoch 46, batch   120] loss: 0.010
[epoch 46, batch   150] loss: 0.003
[epoch 46, batch   180] loss: 0.004
[epoch 46, batch   210] loss: 0.002
[epoch 46, batch   240] loss: 0.013
[epoch 46, batch   270] loss: 0.005
[epoch 46, batch   300] loss: 0.016
[epoch 46, batch   330] loss: 0.008
[epoch 46, batch   360] loss: 0.001
[epoch 46, batch   390] loss: 0.000
[epoch 46, batch   420] loss: 0.000
[epoch 46, batch   450] loss: 0.000
[epoch 46, batch   480] loss: 0.001
[epoch 46, batch   510] loss: 0.001
[epoch 46, batch   540] loss: 0.000
[epoch 46, batch   570] loss: 0.004
[epoch 46, batch   600] loss: 0.001
[epoch 46, batch   630] loss: 0.000
[epoch 46, batch   660] loss: 0.001
[epoch 46, batch   690] loss: 0.008
epoch 46 mean loss: 0.0038
[epoch 47, batch    30] loss: 0.000
[epoch 47, batch    60] loss: 0.007
[epoch 47, batch    90] loss: 0.016
[epoch 47, batch   120] loss: 0.003
[epoch 47, batch   150] loss: 0.007
[epoch 47, batch   180] loss: 0.014
[epoch 47, batch   210] loss: 0.003
[epoch 47, batch   240] loss: 0.000
[epoch 47, batch   270] loss: 0.004
[epoch 47, batch   300] loss: 0.006
[epoch 47, batch   330] loss: 0.001
[epoch 47, batch   360] loss: 0.000
[epoch 47, batch   390] loss: 0.000
[epoch 47, batch   420] loss: 0.001
[epoch 47, batch   450] loss: 0.004
[epoch 47, batch   480] loss: 0.007
[epoch 47, batch   510] loss: 0.000
[epoch 47, batch   540] loss: 0.002
[epoch 47, batch   570] loss: 0.003
[epoch 47, batch   600] loss: 0.004
[epoch 47, batch   630] loss: 0.010
[epoch 47, batch   660] loss: 0.001
[epoch 47, batch   690] loss: 0.002
epoch 47 mean loss: 0.0041
[epoch 48, batch    30] loss: 0.004
[epoch 48, batch    60] loss: 0.007
[epoch 48, batch    90] loss: 0.002
[epoch 48, batch   120] loss: 0.002
[epoch 48, batch   150] loss: 0.016
[epoch 48, batch   180] loss: 0.000
[epoch 48, batch   210] loss: 0.001
[epoch 48, batch   240] loss: 0.002
[epoch 48, batch   270] loss: 0.004
[epoch 48, batch   300] loss: 0.001
[epoch 48, batch   330] loss: 0.001
[epoch 48, batch   360] loss: 0.009
[epoch 48, batch   390] loss: 0.000
[epoch 48, batch   420] loss: 0.001
[epoch 48, batch   450] loss: 0.001
[epoch 48, batch   480] loss: 0.001
[epoch 48, batch   510] loss: 0.008
[epoch 48, batch   540] loss: 0.002
[epoch 48, batch   570] loss: 0.000
[epoch 48, batch   600] loss: 0.001
[epoch 48, batch   630] loss: 0.021
[epoch 48, batch   660] loss: 0.002
[epoch 48, batch   690] loss: 0.003
epoch 48 mean loss: 0.0039
[epoch 49, batch    30] loss: 0.000
[epoch 49, batch    60] loss: 0.001
[epoch 49, batch    90] loss: 0.011
[epoch 49, batch   120] loss: 0.000
[epoch 49, batch   150] loss: 0.009
[epoch 49, batch   180] loss: 0.001
[epoch 49, batch   210] loss: 0.014
[epoch 49, batch   240] loss: 0.004
[epoch 49, batch   270] loss: 0.000
[epoch 49, batch   300] loss: 0.000
[epoch 49, batch   330] loss: 0.005
[epoch 49, batch   360] loss: 0.009
[epoch 49, batch   390] loss: 0.008
[epoch 49, batch   420] loss: 0.002
[epoch 49, batch   450] loss: 0.000
[epoch 49, batch   480] loss: 0.004
[epoch 49, batch   510] loss: 0.006
[epoch 49, batch   540] loss: 0.000
[epoch 49, batch   570] loss: 0.002
[epoch 49, batch   600] loss: 0.002
[epoch 49, batch   630] loss: 0.006
[epoch 49, batch   660] loss: 0.000
[epoch 49, batch   690] loss: 0.000
epoch 49 mean loss: 0.0036
[epoch 50, batch    30] loss: 0.000
[epoch 50, batch    60] loss: 0.000
[epoch 50, batch    90] loss: 0.001
[epoch 50, batch   120] loss: 0.000
[epoch 50, batch   150] loss: 0.008
[epoch 50, batch   180] loss: 0.013
[epoch 50, batch   210] loss: 0.000
[epoch 50, batch   240] loss: 0.000
[epoch 50, batch   270] loss: 0.005
[epoch 50, batch   300] loss: 0.007
[epoch 50, batch   330] loss: 0.000
[epoch 50, batch   360] loss: 0.001
[epoch 50, batch   390] loss: 0.005
[epoch 50, batch   420] loss: 0.002
[epoch 50, batch   450] loss: 0.005
[epoch 50, batch   480] loss: 0.004
[epoch 50, batch   510] loss: 0.003
[epoch 50, batch   540] loss: 0.006
[epoch 50, batch   570] loss: 0.000
[epoch 50, batch   600] loss: 0.003
[epoch 50, batch   630] loss: 0.010
[epoch 50, batch   660] loss: 0.001
[epoch 50, batch   690] loss: 0.002
epoch 50 mean loss: 0.0033
Training Completed.
Debiasing for 2 epochs.
[epoch 1, batch    30] loss: 0.001
[epoch 1, batch    60] loss: 0.000
[epoch 1, batch    90] loss: 0.000
[epoch 1, batch   120] loss: 0.001
[epoch 1, batch   150] loss: 0.006
[epoch 1, batch   180] loss: 0.011
[epoch 1, batch   210] loss: 0.003
[epoch 1, batch   240] loss: 0.004
[epoch 1, batch   270] loss: 0.003
[epoch 1, batch   300] loss: 0.000
[epoch 1, batch   330] loss: 0.001
[epoch 1, batch   360] loss: 0.000
[epoch 1, batch   390] loss: 0.002
[epoch 1, batch   420] loss: 0.004
[epoch 1, batch   450] loss: 0.001
[epoch 1, batch   480] loss: 0.000
[epoch 1, batch   510] loss: 0.001
[epoch 1, batch   540] loss: 0.009
[epoch 1, batch   570] loss: 0.000
[epoch 1, batch   600] loss: 0.001
[epoch 1, batch   630] loss: 0.000
[epoch 1, batch   660] loss: 0.004
[epoch 1, batch   690] loss: 0.002
epoch 1 mean loss: 0.0025
/home/zl22853/code/GeoBS/baselines/main.py:210: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)
  lats, lons = np.radians(loc_te[:, 1]), np.radians(loc_te[:, 0])
[epoch 2, batch    30] loss: 0.013
[epoch 2, batch    60] loss: 0.004
[epoch 2, batch    90] loss: 0.003
[epoch 2, batch   120] loss: 0.000
[epoch 2, batch   150] loss: 0.004
[epoch 2, batch   180] loss: 0.004
[epoch 2, batch   210] loss: 0.010
[epoch 2, batch   240] loss: 0.002
[epoch 2, batch   270] loss: 0.004
[epoch 2, batch   300] loss: 0.005
[epoch 2, batch   330] loss: 0.000
[epoch 2, batch   360] loss: 0.002
[epoch 2, batch   390] loss: 0.003
[epoch 2, batch   420] loss: 0.006
[epoch 2, batch   450] loss: 0.001
[epoch 2, batch   480] loss: 0.004
[epoch 2, batch   510] loss: 0.001
[epoch 2, batch   540] loss: 0.011
[epoch 2, batch   570] loss: 0.001
[epoch 2, batch   600] loss: 0.019
[epoch 2, batch   630] loss: 0.001
[epoch 2, batch   660] loss: 0.002
[epoch 2, batch   690] loss: 0.002
epoch 2 mean loss: 0.0043
Debiasing Completed.
Model saved as TorchSpatial/pre_trained_models/siren(sh)/model_nabirds_ebird_meta_Siren(SH)_trained50_debiased2.pth.tar; in total, trained for 50 epochs, debiased for 2 epochs, in the order of [('train', 50), ('debias', 2)]
Top-1 Accuracy on 24411.0 test images: 77.94%
Top-3 Accuracy on 24411.0 test images: 89.33%
MRR on 24411.0 test images: 0.8432
GBS score on 21058 valid test neighborhoods: 0.1382
