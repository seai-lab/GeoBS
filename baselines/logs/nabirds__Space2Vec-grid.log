/home/zl22853/code/GeoBS/baselines/main.py:150: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)
  lats, lons = np.radians(loc_tr[:,1]), np.radians(loc_tr[:,0])
GPU Name: NVIDIA RTX A6000
Loading nabirds_with_loc_2019.json - train
   using meta data: ebird_meta
	 22819 total entries
	 22819 entries with images
	 22599 entries with meta data
Loading nabirds_with_loc_2019.json - test
   using meta data: ebird_meta
	 24633 total entries
	 24633 entries with images
	 24411 entries with meta data
Check the radian of input data! tensor([-117,   33])
Training for 50 epochs.
[epoch 1, batch    30] loss: 6.308
[epoch 1, batch    60] loss: 6.097
[epoch 1, batch    90] loss: 5.348
[epoch 1, batch   120] loss: 4.212
[epoch 1, batch   150] loss: 2.770
[epoch 1, batch   180] loss: 2.114
[epoch 1, batch   210] loss: 1.583
[epoch 1, batch   240] loss: 1.291
[epoch 1, batch   270] loss: 1.199
[epoch 1, batch   300] loss: 1.122
[epoch 1, batch   330] loss: 1.061
[epoch 1, batch   360] loss: 0.835
[epoch 1, batch   390] loss: 0.791
[epoch 1, batch   420] loss: 0.831
[epoch 1, batch   450] loss: 0.779
[epoch 1, batch   480] loss: 0.798
[epoch 1, batch   510] loss: 0.745
[epoch 1, batch   540] loss: 0.734
[epoch 1, batch   570] loss: 0.709
[epoch 1, batch   600] loss: 0.708
[epoch 1, batch   630] loss: 0.546
[epoch 1, batch   660] loss: 0.706
[epoch 1, batch   690] loss: 0.560
epoch 1 mean loss: 1.7911
[epoch 2, batch    30] loss: 0.471
[epoch 2, batch    60] loss: 0.506
[epoch 2, batch    90] loss: 0.440
[epoch 2, batch   120] loss: 0.523
[epoch 2, batch   150] loss: 0.478
[epoch 2, batch   180] loss: 0.486
[epoch 2, batch   210] loss: 0.534
[epoch 2, batch   240] loss: 0.587
[epoch 2, batch   270] loss: 0.497
[epoch 2, batch   300] loss: 0.483
[epoch 2, batch   330] loss: 0.445
[epoch 2, batch   360] loss: 0.419
[epoch 2, batch   390] loss: 0.520
[epoch 2, batch   420] loss: 0.452
[epoch 2, batch   450] loss: 0.437
[epoch 2, batch   480] loss: 0.498
[epoch 2, batch   510] loss: 0.474
[epoch 2, batch   540] loss: 0.432
[epoch 2, batch   570] loss: 0.370
[epoch 2, batch   600] loss: 0.429
[epoch 2, batch   630] loss: 0.368
[epoch 2, batch   660] loss: 0.391
[epoch 2, batch   690] loss: 0.402
epoch 2 mean loss: 0.4631
[epoch 3, batch    30] loss: 0.299
[epoch 3, batch    60] loss: 0.298
[epoch 3, batch    90] loss: 0.303
[epoch 3, batch   120] loss: 0.340
[epoch 3, batch   150] loss: 0.303
[epoch 3, batch   180] loss: 0.343
[epoch 3, batch   210] loss: 0.305
[epoch 3, batch   240] loss: 0.353
[epoch 3, batch   270] loss: 0.427
[epoch 3, batch   300] loss: 0.326
[epoch 3, batch   330] loss: 0.310
[epoch 3, batch   360] loss: 0.268
[epoch 3, batch   390] loss: 0.351
[epoch 3, batch   420] loss: 0.308
[epoch 3, batch   450] loss: 0.322
[epoch 3, batch   480] loss: 0.237
[epoch 3, batch   510] loss: 0.285
[epoch 3, batch   540] loss: 0.354
[epoch 3, batch   570] loss: 0.256
[epoch 3, batch   600] loss: 0.373
[epoch 3, batch   630] loss: 0.220
[epoch 3, batch   660] loss: 0.321
[epoch 3, batch   690] loss: 0.381
epoch 3 mean loss: 0.3162
[epoch 4, batch    30] loss: 0.289
[epoch 4, batch    60] loss: 0.235
[epoch 4, batch    90] loss: 0.189
[epoch 4, batch   120] loss: 0.349
[epoch 4, batch   150] loss: 0.259
[epoch 4, batch   180] loss: 0.324
[epoch 4, batch   210] loss: 0.224
[epoch 4, batch   240] loss: 0.238
[epoch 4, batch   270] loss: 0.200
[epoch 4, batch   300] loss: 0.246
[epoch 4, batch   330] loss: 0.269
[epoch 4, batch   360] loss: 0.231
[epoch 4, batch   390] loss: 0.224
[epoch 4, batch   420] loss: 0.203
[epoch 4, batch   450] loss: 0.186
[epoch 4, batch   480] loss: 0.262
[epoch 4, batch   510] loss: 0.258
[epoch 4, batch   540] loss: 0.267
[epoch 4, batch   570] loss: 0.244
[epoch 4, batch   600] loss: 0.241
[epoch 4, batch   630] loss: 0.248
[epoch 4, batch   660] loss: 0.245
[epoch 4, batch   690] loss: 0.246
epoch 4 mean loss: 0.2482
[epoch 5, batch    30] loss: 0.242
[epoch 5, batch    60] loss: 0.235
[epoch 5, batch    90] loss: 0.234
[epoch 5, batch   120] loss: 0.223
[epoch 5, batch   150] loss: 0.251
[epoch 5, batch   180] loss: 0.205
[epoch 5, batch   210] loss: 0.232
[epoch 5, batch   240] loss: 0.224
[epoch 5, batch   270] loss: 0.159
[epoch 5, batch   300] loss: 0.216
[epoch 5, batch   330] loss: 0.238
[epoch 5, batch   360] loss: 0.211
[epoch 5, batch   390] loss: 0.228
[epoch 5, batch   420] loss: 0.216
[epoch 5, batch   450] loss: 0.225
[epoch 5, batch   480] loss: 0.225
[epoch 5, batch   510] loss: 0.189
[epoch 5, batch   540] loss: 0.231
[epoch 5, batch   570] loss: 0.131
[epoch 5, batch   600] loss: 0.217
[epoch 5, batch   630] loss: 0.245
[epoch 5, batch   660] loss: 0.160
[epoch 5, batch   690] loss: 0.192
epoch 5 mean loss: 0.2125
[epoch 6, batch    30] loss: 0.161
[epoch 6, batch    60] loss: 0.175
[epoch 6, batch    90] loss: 0.130
[epoch 6, batch   120] loss: 0.178
[epoch 6, batch   150] loss: 0.145
[epoch 6, batch   180] loss: 0.218
[epoch 6, batch   210] loss: 0.186
[epoch 6, batch   240] loss: 0.194
[epoch 6, batch   270] loss: 0.150
[epoch 6, batch   300] loss: 0.118
[epoch 6, batch   330] loss: 0.148
[epoch 6, batch   360] loss: 0.179
[epoch 6, batch   390] loss: 0.204
[epoch 6, batch   420] loss: 0.182
[epoch 6, batch   450] loss: 0.164
[epoch 6, batch   480] loss: 0.188
[epoch 6, batch   510] loss: 0.180
[epoch 6, batch   540] loss: 0.133
[epoch 6, batch   570] loss: 0.199
[epoch 6, batch   600] loss: 0.218
[epoch 6, batch   630] loss: 0.224
[epoch 6, batch   660] loss: 0.150
[epoch 6, batch   690] loss: 0.168
epoch 6 mean loss: 0.1731
[epoch 7, batch    30] loss: 0.119
[epoch 7, batch    60] loss: 0.166
[epoch 7, batch    90] loss: 0.132
[epoch 7, batch   120] loss: 0.156
[epoch 7, batch   150] loss: 0.111
[epoch 7, batch   180] loss: 0.128
[epoch 7, batch   210] loss: 0.232
[epoch 7, batch   240] loss: 0.121
[epoch 7, batch   270] loss: 0.121
[epoch 7, batch   300] loss: 0.137
[epoch 7, batch   330] loss: 0.137
[epoch 7, batch   360] loss: 0.123
[epoch 7, batch   390] loss: 0.158
[epoch 7, batch   420] loss: 0.134
[epoch 7, batch   450] loss: 0.171
[epoch 7, batch   480] loss: 0.164
[epoch 7, batch   510] loss: 0.137
[epoch 7, batch   540] loss: 0.144
[epoch 7, batch   570] loss: 0.167
[epoch 7, batch   600] loss: 0.101
[epoch 7, batch   630] loss: 0.183
[epoch 7, batch   660] loss: 0.166
[epoch 7, batch   690] loss: 0.116
epoch 7 mean loss: 0.1456
[epoch 8, batch    30] loss: 0.122
[epoch 8, batch    60] loss: 0.129
[epoch 8, batch    90] loss: 0.178
[epoch 8, batch   120] loss: 0.125
[epoch 8, batch   150] loss: 0.155
[epoch 8, batch   180] loss: 0.119
[epoch 8, batch   210] loss: 0.128
[epoch 8, batch   240] loss: 0.142
[epoch 8, batch   270] loss: 0.144
[epoch 8, batch   300] loss: 0.151
[epoch 8, batch   330] loss: 0.137
[epoch 8, batch   360] loss: 0.144
[epoch 8, batch   390] loss: 0.140
[epoch 8, batch   420] loss: 0.120
[epoch 8, batch   450] loss: 0.131
[epoch 8, batch   480] loss: 0.112
[epoch 8, batch   510] loss: 0.113
[epoch 8, batch   540] loss: 0.167
[epoch 8, batch   570] loss: 0.139
[epoch 8, batch   600] loss: 0.125
[epoch 8, batch   630] loss: 0.160
[epoch 8, batch   660] loss: 0.119
[epoch 8, batch   690] loss: 0.168
epoch 8 mean loss: 0.1382
[epoch 9, batch    30] loss: 0.113
[epoch 9, batch    60] loss: 0.118
[epoch 9, batch    90] loss: 0.082
[epoch 9, batch   120] loss: 0.161
[epoch 9, batch   150] loss: 0.148
[epoch 9, batch   180] loss: 0.108
[epoch 9, batch   210] loss: 0.125
[epoch 9, batch   240] loss: 0.122
[epoch 9, batch   270] loss: 0.117
[epoch 9, batch   300] loss: 0.082
[epoch 9, batch   330] loss: 0.102
[epoch 9, batch   360] loss: 0.115
[epoch 9, batch   390] loss: 0.087
[epoch 9, batch   420] loss: 0.142
[epoch 9, batch   450] loss: 0.157
[epoch 9, batch   480] loss: 0.102
[epoch 9, batch   510] loss: 0.134
[epoch 9, batch   540] loss: 0.096
[epoch 9, batch   570] loss: 0.106
[epoch 9, batch   600] loss: 0.139
[epoch 9, batch   630] loss: 0.123
[epoch 9, batch   660] loss: 0.114
[epoch 9, batch   690] loss: 0.124
epoch 9 mean loss: 0.1196
[epoch 10, batch    30] loss: 0.104
[epoch 10, batch    60] loss: 0.090
[epoch 10, batch    90] loss: 0.099
[epoch 10, batch   120] loss: 0.071
[epoch 10, batch   150] loss: 0.088
[epoch 10, batch   180] loss: 0.082
[epoch 10, batch   210] loss: 0.073
[epoch 10, batch   240] loss: 0.157
[epoch 10, batch   270] loss: 0.077
[epoch 10, batch   300] loss: 0.135
[epoch 10, batch   330] loss: 0.096
[epoch 10, batch   360] loss: 0.102
[epoch 10, batch   390] loss: 0.123
[epoch 10, batch   420] loss: 0.098
[epoch 10, batch   450] loss: 0.128
[epoch 10, batch   480] loss: 0.102
[epoch 10, batch   510] loss: 0.106
[epoch 10, batch   540] loss: 0.100
[epoch 10, batch   570] loss: 0.102
[epoch 10, batch   600] loss: 0.114
[epoch 10, batch   630] loss: 0.118
[epoch 10, batch   660] loss: 0.102
[epoch 10, batch   690] loss: 0.092
epoch 10 mean loss: 0.1028
[epoch 11, batch    30] loss: 0.115
[epoch 11, batch    60] loss: 0.085
[epoch 11, batch    90] loss: 0.094
[epoch 11, batch   120] loss: 0.111
[epoch 11, batch   150] loss: 0.089
[epoch 11, batch   180] loss: 0.062
[epoch 11, batch   210] loss: 0.070
[epoch 11, batch   240] loss: 0.077
[epoch 11, batch   270] loss: 0.085
[epoch 11, batch   300] loss: 0.105
[epoch 11, batch   330] loss: 0.103
[epoch 11, batch   360] loss: 0.103
[epoch 11, batch   390] loss: 0.087
[epoch 11, batch   420] loss: 0.122
[epoch 11, batch   450] loss: 0.087
[epoch 11, batch   480] loss: 0.130
[epoch 11, batch   510] loss: 0.103
[epoch 11, batch   540] loss: 0.085
[epoch 11, batch   570] loss: 0.077
[epoch 11, batch   600] loss: 0.091
[epoch 11, batch   630] loss: 0.082
[epoch 11, batch   660] loss: 0.082
[epoch 11, batch   690] loss: 0.095
epoch 11 mean loss: 0.0949
[epoch 12, batch    30] loss: 0.123
[epoch 12, batch    60] loss: 0.078
[epoch 12, batch    90] loss: 0.072
[epoch 12, batch   120] loss: 0.062
[epoch 12, batch   150] loss: 0.096
[epoch 12, batch   180] loss: 0.106
[epoch 12, batch   210] loss: 0.136
[epoch 12, batch   240] loss: 0.080
[epoch 12, batch   270] loss: 0.059
[epoch 12, batch   300] loss: 0.102
[epoch 12, batch   330] loss: 0.090
[epoch 12, batch   360] loss: 0.064
[epoch 12, batch   390] loss: 0.087
[epoch 12, batch   420] loss: 0.090
[epoch 12, batch   450] loss: 0.084
[epoch 12, batch   480] loss: 0.088
[epoch 12, batch   510] loss: 0.084
[epoch 12, batch   540] loss: 0.105
[epoch 12, batch   570] loss: 0.084
[epoch 12, batch   600] loss: 0.101
[epoch 12, batch   630] loss: 0.082
[epoch 12, batch   660] loss: 0.138
[epoch 12, batch   690] loss: 0.090
epoch 12 mean loss: 0.0916
[epoch 13, batch    30] loss: 0.100
[epoch 13, batch    60] loss: 0.068
[epoch 13, batch    90] loss: 0.102
[epoch 13, batch   120] loss: 0.068
[epoch 13, batch   150] loss: 0.087
[epoch 13, batch   180] loss: 0.074
[epoch 13, batch   210] loss: 0.083
[epoch 13, batch   240] loss: 0.071
[epoch 13, batch   270] loss: 0.136
[epoch 13, batch   300] loss: 0.098
[epoch 13, batch   330] loss: 0.067
[epoch 13, batch   360] loss: 0.064
[epoch 13, batch   390] loss: 0.130
[epoch 13, batch   420] loss: 0.100
[epoch 13, batch   450] loss: 0.090
[epoch 13, batch   480] loss: 0.089
[epoch 13, batch   510] loss: 0.096
[epoch 13, batch   540] loss: 0.106
[epoch 13, batch   570] loss: 0.096
[epoch 13, batch   600] loss: 0.082
[epoch 13, batch   630] loss: 0.102
[epoch 13, batch   660] loss: 0.113
[epoch 13, batch   690] loss: 0.096
epoch 13 mean loss: 0.0924
[epoch 14, batch    30] loss: 0.169
[epoch 14, batch    60] loss: 0.118
[epoch 14, batch    90] loss: 0.093
[epoch 14, batch   120] loss: 0.044
[epoch 14, batch   150] loss: 0.059
[epoch 14, batch   180] loss: 0.090
[epoch 14, batch   210] loss: 0.050
[epoch 14, batch   240] loss: 0.073
[epoch 14, batch   270] loss: 0.050
[epoch 14, batch   300] loss: 0.079
[epoch 14, batch   330] loss: 0.109
[epoch 14, batch   360] loss: 0.068
[epoch 14, batch   390] loss: 0.162
[epoch 14, batch   420] loss: 0.088
[epoch 14, batch   450] loss: 0.070
[epoch 14, batch   480] loss: 0.062
[epoch 14, batch   510] loss: 0.136
[epoch 14, batch   540] loss: 0.081
[epoch 14, batch   570] loss: 0.086
[epoch 14, batch   600] loss: 0.062
[epoch 14, batch   630] loss: 0.075
[epoch 14, batch   660] loss: 0.104
[epoch 14, batch   690] loss: 0.067
epoch 14 mean loss: 0.0872
[epoch 15, batch    30] loss: 0.090
[epoch 15, batch    60] loss: 0.075
[epoch 15, batch    90] loss: 0.089
[epoch 15, batch   120] loss: 0.090
[epoch 15, batch   150] loss: 0.051
[epoch 15, batch   180] loss: 0.074
[epoch 15, batch   210] loss: 0.101
[epoch 15, batch   240] loss: 0.073
[epoch 15, batch   270] loss: 0.062
[epoch 15, batch   300] loss: 0.086
[epoch 15, batch   330] loss: 0.083
[epoch 15, batch   360] loss: 0.089
[epoch 15, batch   390] loss: 0.088
[epoch 15, batch   420] loss: 0.088
[epoch 15, batch   450] loss: 0.068
[epoch 15, batch   480] loss: 0.086
[epoch 15, batch   510] loss: 0.050
[epoch 15, batch   540] loss: 0.104
[epoch 15, batch   570] loss: 0.075
[epoch 15, batch   600] loss: 0.094
[epoch 15, batch   630] loss: 0.087
[epoch 15, batch   660] loss: 0.119
[epoch 15, batch   690] loss: 0.085
epoch 15 mean loss: 0.0821
[epoch 16, batch    30] loss: 0.088
[epoch 16, batch    60] loss: 0.043
[epoch 16, batch    90] loss: 0.053
[epoch 16, batch   120] loss: 0.076
[epoch 16, batch   150] loss: 0.082
[epoch 16, batch   180] loss: 0.055
[epoch 16, batch   210] loss: 0.066
[epoch 16, batch   240] loss: 0.068
[epoch 16, batch   270] loss: 0.064
[epoch 16, batch   300] loss: 0.105
[epoch 16, batch   330] loss: 0.091
[epoch 16, batch   360] loss: 0.084
[epoch 16, batch   390] loss: 0.071
[epoch 16, batch   420] loss: 0.071
[epoch 16, batch   450] loss: 0.049
[epoch 16, batch   480] loss: 0.074
[epoch 16, batch   510] loss: 0.129
[epoch 16, batch   540] loss: 0.083
[epoch 16, batch   570] loss: 0.100
[epoch 16, batch   600] loss: 0.102
[epoch 16, batch   630] loss: 0.092
[epoch 16, batch   660] loss: 0.114
[epoch 16, batch   690] loss: 0.090
epoch 16 mean loss: 0.0803
[epoch 17, batch    30] loss: 0.043
[epoch 17, batch    60] loss: 0.108
[epoch 17, batch    90] loss: 0.084
[epoch 17, batch   120] loss: 0.094
[epoch 17, batch   150] loss: 0.075
[epoch 17, batch   180] loss: 0.065
[epoch 17, batch   210] loss: 0.072
[epoch 17, batch   240] loss: 0.096
[epoch 17, batch   270] loss: 0.121
[epoch 17, batch   300] loss: 0.080
[epoch 17, batch   330] loss: 0.051
[epoch 17, batch   360] loss: 0.053
[epoch 17, batch   390] loss: 0.065
[epoch 17, batch   420] loss: 0.079
[epoch 17, batch   450] loss: 0.055
[epoch 17, batch   480] loss: 0.038
[epoch 17, batch   510] loss: 0.051
[epoch 17, batch   540] loss: 0.089
[epoch 17, batch   570] loss: 0.102
[epoch 17, batch   600] loss: 0.068
[epoch 17, batch   630] loss: 0.082
[epoch 17, batch   660] loss: 0.075
[epoch 17, batch   690] loss: 0.059
epoch 17 mean loss: 0.0735
[epoch 18, batch    30] loss: 0.078
[epoch 18, batch    60] loss: 0.129
[epoch 18, batch    90] loss: 0.053
[epoch 18, batch   120] loss: 0.072
[epoch 18, batch   150] loss: 0.080
[epoch 18, batch   180] loss: 0.090
[epoch 18, batch   210] loss: 0.055
[epoch 18, batch   240] loss: 0.068
[epoch 18, batch   270] loss: 0.084
[epoch 18, batch   300] loss: 0.077
[epoch 18, batch   330] loss: 0.048
[epoch 18, batch   360] loss: 0.079
[epoch 18, batch   390] loss: 0.048
[epoch 18, batch   420] loss: 0.107
[epoch 18, batch   450] loss: 0.131
[epoch 18, batch   480] loss: 0.098
[epoch 18, batch   510] loss: 0.106
[epoch 18, batch   540] loss: 0.074
[epoch 18, batch   570] loss: 0.065
[epoch 18, batch   600] loss: 0.091
[epoch 18, batch   630] loss: 0.085
[epoch 18, batch   660] loss: 0.073
[epoch 18, batch   690] loss: 0.080
epoch 18 mean loss: 0.0803
[epoch 19, batch    30] loss: 0.095
[epoch 19, batch    60] loss: 0.034
[epoch 19, batch    90] loss: 0.042
[epoch 19, batch   120] loss: 0.064
[epoch 19, batch   150] loss: 0.048
[epoch 19, batch   180] loss: 0.065
[epoch 19, batch   210] loss: 0.070
[epoch 19, batch   240] loss: 0.046
[epoch 19, batch   270] loss: 0.075
[epoch 19, batch   300] loss: 0.053
[epoch 19, batch   330] loss: 0.072
[epoch 19, batch   360] loss: 0.058
[epoch 19, batch   390] loss: 0.058
[epoch 19, batch   420] loss: 0.069
[epoch 19, batch   450] loss: 0.061
[epoch 19, batch   480] loss: 0.075
[epoch 19, batch   510] loss: 0.074
[epoch 19, batch   540] loss: 0.114
[epoch 19, batch   570] loss: 0.072
[epoch 19, batch   600] loss: 0.042
[epoch 19, batch   630] loss: 0.059
[epoch 19, batch   660] loss: 0.060
[epoch 19, batch   690] loss: 0.074
epoch 19 mean loss: 0.0643
[epoch 20, batch    30] loss: 0.028
[epoch 20, batch    60] loss: 0.042
[epoch 20, batch    90] loss: 0.042
[epoch 20, batch   120] loss: 0.087
[epoch 20, batch   150] loss: 0.033
[epoch 20, batch   180] loss: 0.083
[epoch 20, batch   210] loss: 0.046
[epoch 20, batch   240] loss: 0.070
[epoch 20, batch   270] loss: 0.050
[epoch 20, batch   300] loss: 0.065
[epoch 20, batch   330] loss: 0.110
[epoch 20, batch   360] loss: 0.043
[epoch 20, batch   390] loss: 0.054
[epoch 20, batch   420] loss: 0.081
[epoch 20, batch   450] loss: 0.031
[epoch 20, batch   480] loss: 0.084
[epoch 20, batch   510] loss: 0.049
[epoch 20, batch   540] loss: 0.123
[epoch 20, batch   570] loss: 0.033
[epoch 20, batch   600] loss: 0.114
[epoch 20, batch   630] loss: 0.060
[epoch 20, batch   660] loss: 0.067
[epoch 20, batch   690] loss: 0.081
epoch 20 mean loss: 0.0640
[epoch 21, batch    30] loss: 0.036
[epoch 21, batch    60] loss: 0.061
[epoch 21, batch    90] loss: 0.084
[epoch 21, batch   120] loss: 0.073
[epoch 21, batch   150] loss: 0.058
[epoch 21, batch   180] loss: 0.087
[epoch 21, batch   210] loss: 0.045
[epoch 21, batch   240] loss: 0.025
[epoch 21, batch   270] loss: 0.038
[epoch 21, batch   300] loss: 0.038
[epoch 21, batch   330] loss: 0.073
[epoch 21, batch   360] loss: 0.100
[epoch 21, batch   390] loss: 0.041
[epoch 21, batch   420] loss: 0.028
[epoch 21, batch   450] loss: 0.091
[epoch 21, batch   480] loss: 0.098
[epoch 21, batch   510] loss: 0.067
[epoch 21, batch   540] loss: 0.118
[epoch 21, batch   570] loss: 0.040
[epoch 21, batch   600] loss: 0.060
[epoch 21, batch   630] loss: 0.102
[epoch 21, batch   660] loss: 0.057
[epoch 21, batch   690] loss: 0.043
epoch 21 mean loss: 0.0635
[epoch 22, batch    30] loss: 0.120
[epoch 22, batch    60] loss: 0.070
[epoch 22, batch    90] loss: 0.063
[epoch 22, batch   120] loss: 0.114
[epoch 22, batch   150] loss: 0.064
[epoch 22, batch   180] loss: 0.032
[epoch 22, batch   210] loss: 0.049
[epoch 22, batch   240] loss: 0.063
[epoch 22, batch   270] loss: 0.091
[epoch 22, batch   300] loss: 0.068
[epoch 22, batch   330] loss: 0.127
[epoch 22, batch   360] loss: 0.053
[epoch 22, batch   390] loss: 0.108
[epoch 22, batch   420] loss: 0.074
[epoch 22, batch   450] loss: 0.078
[epoch 22, batch   480] loss: 0.084
[epoch 22, batch   510] loss: 0.058
[epoch 22, batch   540] loss: 0.067
[epoch 22, batch   570] loss: 0.057
[epoch 22, batch   600] loss: 0.078
[epoch 22, batch   630] loss: 0.039
[epoch 22, batch   660] loss: 0.061
[epoch 22, batch   690] loss: 0.097
epoch 22 mean loss: 0.0748
[epoch 23, batch    30] loss: 0.074
[epoch 23, batch    60] loss: 0.068
[epoch 23, batch    90] loss: 0.051
[epoch 23, batch   120] loss: 0.061
[epoch 23, batch   150] loss: 0.033
[epoch 23, batch   180] loss: 0.067
[epoch 23, batch   210] loss: 0.038
[epoch 23, batch   240] loss: 0.060
[epoch 23, batch   270] loss: 0.040
[epoch 23, batch   300] loss: 0.031
[epoch 23, batch   330] loss: 0.051
[epoch 23, batch   360] loss: 0.070
[epoch 23, batch   390] loss: 0.049
[epoch 23, batch   420] loss: 0.080
[epoch 23, batch   450] loss: 0.068
[epoch 23, batch   480] loss: 0.050
[epoch 23, batch   510] loss: 0.039
[epoch 23, batch   540] loss: 0.099
[epoch 23, batch   570] loss: 0.062
[epoch 23, batch   600] loss: 0.077
[epoch 23, batch   630] loss: 0.091
[epoch 23, batch   660] loss: 0.060
[epoch 23, batch   690] loss: 0.041
epoch 23 mean loss: 0.0599
[epoch 24, batch    30] loss: 0.057
[epoch 24, batch    60] loss: 0.039
[epoch 24, batch    90] loss: 0.079
[epoch 24, batch   120] loss: 0.045
[epoch 24, batch   150] loss: 0.106
[epoch 24, batch   180] loss: 0.051
[epoch 24, batch   210] loss: 0.085
[epoch 24, batch   240] loss: 0.096
[epoch 24, batch   270] loss: 0.078
[epoch 24, batch   300] loss: 0.040
[epoch 24, batch   330] loss: 0.063
[epoch 24, batch   360] loss: 0.054
[epoch 24, batch   390] loss: 0.058
[epoch 24, batch   420] loss: 0.063
[epoch 24, batch   450] loss: 0.044
[epoch 24, batch   480] loss: 0.031
[epoch 24, batch   510] loss: 0.068
[epoch 24, batch   540] loss: 0.048
[epoch 24, batch   570] loss: 0.028
[epoch 24, batch   600] loss: 0.064
[epoch 24, batch   630] loss: 0.049
[epoch 24, batch   660] loss: 0.117
[epoch 24, batch   690] loss: 0.082
epoch 24 mean loss: 0.0625
[epoch 25, batch    30] loss: 0.048
[epoch 25, batch    60] loss: 0.070
[epoch 25, batch    90] loss: 0.045
[epoch 25, batch   120] loss: 0.043
[epoch 25, batch   150] loss: 0.044
[epoch 25, batch   180] loss: 0.071
[epoch 25, batch   210] loss: 0.037
[epoch 25, batch   240] loss: 0.073
[epoch 25, batch   270] loss: 0.090
[epoch 25, batch   300] loss: 0.085
[epoch 25, batch   330] loss: 0.087
[epoch 25, batch   360] loss: 0.046
[epoch 25, batch   390] loss: 0.054
[epoch 25, batch   420] loss: 0.061
[epoch 25, batch   450] loss: 0.019
[epoch 25, batch   480] loss: 0.070
[epoch 25, batch   510] loss: 0.067
[epoch 25, batch   540] loss: 0.074
[epoch 25, batch   570] loss: 0.049
[epoch 25, batch   600] loss: 0.041
[epoch 25, batch   630] loss: 0.040
[epoch 25, batch   660] loss: 0.056
[epoch 25, batch   690] loss: 0.073
epoch 25 mean loss: 0.0592
[epoch 26, batch    30] loss: 0.055
[epoch 26, batch    60] loss: 0.045
[epoch 26, batch    90] loss: 0.066
[epoch 26, batch   120] loss: 0.028
[epoch 26, batch   150] loss: 0.095
[epoch 26, batch   180] loss: 0.061
[epoch 26, batch   210] loss: 0.092
[epoch 26, batch   240] loss: 0.086
[epoch 26, batch   270] loss: 0.037
[epoch 26, batch   300] loss: 0.032
[epoch 26, batch   330] loss: 0.049
[epoch 26, batch   360] loss: 0.056
[epoch 26, batch   390] loss: 0.093
[epoch 26, batch   420] loss: 0.062
[epoch 26, batch   450] loss: 0.036
[epoch 26, batch   480] loss: 0.063
[epoch 26, batch   510] loss: 0.064
[epoch 26, batch   540] loss: 0.042
[epoch 26, batch   570] loss: 0.059
[epoch 26, batch   600] loss: 0.074
[epoch 26, batch   630] loss: 0.081
[epoch 26, batch   660] loss: 0.080
[epoch 26, batch   690] loss: 0.037
epoch 26 mean loss: 0.0631
[epoch 27, batch    30] loss: 0.045
[epoch 27, batch    60] loss: 0.066
[epoch 27, batch    90] loss: 0.033
[epoch 27, batch   120] loss: 0.066
[epoch 27, batch   150] loss: 0.093
[epoch 27, batch   180] loss: 0.086
[epoch 27, batch   210] loss: 0.053
[epoch 27, batch   240] loss: 0.041
[epoch 27, batch   270] loss: 0.062
[epoch 27, batch   300] loss: 0.083
[epoch 27, batch   330] loss: 0.084
[epoch 27, batch   360] loss: 0.046
[epoch 27, batch   390] loss: 0.046
[epoch 27, batch   420] loss: 0.066
[epoch 27, batch   450] loss: 0.086
[epoch 27, batch   480] loss: 0.066
[epoch 27, batch   510] loss: 0.018
[epoch 27, batch   540] loss: 0.086
[epoch 27, batch   570] loss: 0.062
[epoch 27, batch   600] loss: 0.066
[epoch 27, batch   630] loss: 0.050
[epoch 27, batch   660] loss: 0.076
[epoch 27, batch   690] loss: 0.052
epoch 27 mean loss: 0.0623
[epoch 28, batch    30] loss: 0.065
[epoch 28, batch    60] loss: 0.057
[epoch 28, batch    90] loss: 0.041
[epoch 28, batch   120] loss: 0.087
[epoch 28, batch   150] loss: 0.037
[epoch 28, batch   180] loss: 0.034
[epoch 28, batch   210] loss: 0.075
[epoch 28, batch   240] loss: 0.071
[epoch 28, batch   270] loss: 0.037
[epoch 28, batch   300] loss: 0.095
[epoch 28, batch   330] loss: 0.053
[epoch 28, batch   360] loss: 0.043
[epoch 28, batch   390] loss: 0.061
[epoch 28, batch   420] loss: 0.038
[epoch 28, batch   450] loss: 0.067
[epoch 28, batch   480] loss: 0.061
[epoch 28, batch   510] loss: 0.055
[epoch 28, batch   540] loss: 0.081
[epoch 28, batch   570] loss: 0.050
[epoch 28, batch   600] loss: 0.073
[epoch 28, batch   630] loss: 0.058
[epoch 28, batch   660] loss: 0.072
[epoch 28, batch   690] loss: 0.054
epoch 28 mean loss: 0.0586
[epoch 29, batch    30] loss: 0.074
[epoch 29, batch    60] loss: 0.044
[epoch 29, batch    90] loss: 0.049
[epoch 29, batch   120] loss: 0.045
[epoch 29, batch   150] loss: 0.097
[epoch 29, batch   180] loss: 0.052
[epoch 29, batch   210] loss: 0.074
[epoch 29, batch   240] loss: 0.037
[epoch 29, batch   270] loss: 0.054
[epoch 29, batch   300] loss: 0.044
[epoch 29, batch   330] loss: 0.046
[epoch 29, batch   360] loss: 0.071
[epoch 29, batch   390] loss: 0.043
[epoch 29, batch   420] loss: 0.054
[epoch 29, batch   450] loss: 0.037
[epoch 29, batch   480] loss: 0.087
[epoch 29, batch   510] loss: 0.078
[epoch 29, batch   540] loss: 0.039
[epoch 29, batch   570] loss: 0.032
[epoch 29, batch   600] loss: 0.052
[epoch 29, batch   630] loss: 0.070
[epoch 29, batch   660] loss: 0.067
[epoch 29, batch   690] loss: 0.066
epoch 29 mean loss: 0.0575
[epoch 30, batch    30] loss: 0.028
[epoch 30, batch    60] loss: 0.058
[epoch 30, batch    90] loss: 0.064
[epoch 30, batch   120] loss: 0.045
[epoch 30, batch   150] loss: 0.048
[epoch 30, batch   180] loss: 0.007
[epoch 30, batch   210] loss: 0.067
[epoch 30, batch   240] loss: 0.086
[epoch 30, batch   270] loss: 0.083
[epoch 30, batch   300] loss: 0.030
[epoch 30, batch   330] loss: 0.090
[epoch 30, batch   360] loss: 0.053
[epoch 30, batch   390] loss: 0.075
[epoch 30, batch   420] loss: 0.072
[epoch 30, batch   450] loss: 0.057
[epoch 30, batch   480] loss: 0.112
[epoch 30, batch   510] loss: 0.044
[epoch 30, batch   540] loss: 0.032
[epoch 30, batch   570] loss: 0.032
[epoch 30, batch   600] loss: 0.084
[epoch 30, batch   630] loss: 0.075
[epoch 30, batch   660] loss: 0.070
[epoch 30, batch   690] loss: 0.075
epoch 30 mean loss: 0.0605
[epoch 31, batch    30] loss: 0.046
[epoch 31, batch    60] loss: 0.059
[epoch 31, batch    90] loss: 0.028
[epoch 31, batch   120] loss: 0.025
[epoch 31, batch   150] loss: 0.049
[epoch 31, batch   180] loss: 0.026
[epoch 31, batch   210] loss: 0.017
[epoch 31, batch   240] loss: 0.039
[epoch 31, batch   270] loss: 0.066
[epoch 31, batch   300] loss: 0.066
[epoch 31, batch   330] loss: 0.082
[epoch 31, batch   360] loss: 0.041
[epoch 31, batch   390] loss: 0.045
[epoch 31, batch   420] loss: 0.045
[epoch 31, batch   450] loss: 0.053
[epoch 31, batch   480] loss: 0.036
[epoch 31, batch   510] loss: 0.058
[epoch 31, batch   540] loss: 0.056
[epoch 31, batch   570] loss: 0.164
[epoch 31, batch   600] loss: 0.042
[epoch 31, batch   630] loss: 0.073
[epoch 31, batch   660] loss: 0.066
[epoch 31, batch   690] loss: 0.053
epoch 31 mean loss: 0.0540
[epoch 32, batch    30] loss: 0.084
[epoch 32, batch    60] loss: 0.056
[epoch 32, batch    90] loss: 0.062
[epoch 32, batch   120] loss: 0.069
[epoch 32, batch   150] loss: 0.039
[epoch 32, batch   180] loss: 0.073
[epoch 32, batch   210] loss: 0.069
[epoch 32, batch   240] loss: 0.048
[epoch 32, batch   270] loss: 0.027
[epoch 32, batch   300] loss: 0.014
[epoch 32, batch   330] loss: 0.045
[epoch 32, batch   360] loss: 0.073
[epoch 32, batch   390] loss: 0.028
[epoch 32, batch   420] loss: 0.029
[epoch 32, batch   450] loss: 0.035
[epoch 32, batch   480] loss: 0.044
[epoch 32, batch   510] loss: 0.041
[epoch 32, batch   540] loss: 0.043
[epoch 32, batch   570] loss: 0.020
[epoch 32, batch   600] loss: 0.054
[epoch 32, batch   630] loss: 0.022
[epoch 32, batch   660] loss: 0.034
[epoch 32, batch   690] loss: 0.064
epoch 32 mean loss: 0.0464
[epoch 33, batch    30] loss: 0.056
[epoch 33, batch    60] loss: 0.065
[epoch 33, batch    90] loss: 0.059
[epoch 33, batch   120] loss: 0.055
[epoch 33, batch   150] loss: 0.058
[epoch 33, batch   180] loss: 0.060
[epoch 33, batch   210] loss: 0.044
[epoch 33, batch   240] loss: 0.027
[epoch 33, batch   270] loss: 0.036
[epoch 33, batch   300] loss: 0.035
[epoch 33, batch   330] loss: 0.085
[epoch 33, batch   360] loss: 0.035
[epoch 33, batch   390] loss: 0.032
[epoch 33, batch   420] loss: 0.076
[epoch 33, batch   450] loss: 0.048
[epoch 33, batch   480] loss: 0.043
[epoch 33, batch   510] loss: 0.061
[epoch 33, batch   540] loss: 0.034
[epoch 33, batch   570] loss: 0.060
[epoch 33, batch   600] loss: 0.040
[epoch 33, batch   630] loss: 0.090
[epoch 33, batch   660] loss: 0.052
[epoch 33, batch   690] loss: 0.049
epoch 33 mean loss: 0.0525
[epoch 34, batch    30] loss: 0.023
[epoch 34, batch    60] loss: 0.042
[epoch 34, batch    90] loss: 0.032
[epoch 34, batch   120] loss: 0.052
[epoch 34, batch   150] loss: 0.007
[epoch 34, batch   180] loss: 0.086
[epoch 34, batch   210] loss: 0.043
[epoch 34, batch   240] loss: 0.063
[epoch 34, batch   270] loss: 0.018
[epoch 34, batch   300] loss: 0.036
[epoch 34, batch   330] loss: 0.067
[epoch 34, batch   360] loss: 0.034
[epoch 34, batch   390] loss: 0.056
[epoch 34, batch   420] loss: 0.050
[epoch 34, batch   450] loss: 0.081
[epoch 34, batch   480] loss: 0.025
[epoch 34, batch   510] loss: 0.073
[epoch 34, batch   540] loss: 0.070
[epoch 34, batch   570] loss: 0.034
[epoch 34, batch   600] loss: 0.038
[epoch 34, batch   630] loss: 0.072
[epoch 34, batch   660] loss: 0.052
[epoch 34, batch   690] loss: 0.042
epoch 34 mean loss: 0.0469
[epoch 35, batch    30] loss: 0.044
[epoch 35, batch    60] loss: 0.083
[epoch 35, batch    90] loss: 0.068
[epoch 35, batch   120] loss: 0.045
[epoch 35, batch   150] loss: 0.069
[epoch 35, batch   180] loss: 0.028
[epoch 35, batch   210] loss: 0.063
[epoch 35, batch   240] loss: 0.112
[epoch 35, batch   270] loss: 0.031
[epoch 35, batch   300] loss: 0.067
[epoch 35, batch   330] loss: 0.044
[epoch 35, batch   360] loss: 0.027
[epoch 35, batch   390] loss: 0.069
[epoch 35, batch   420] loss: 0.089
[epoch 35, batch   450] loss: 0.052
[epoch 35, batch   480] loss: 0.074
[epoch 35, batch   510] loss: 0.069
[epoch 35, batch   540] loss: 0.080
[epoch 35, batch   570] loss: 0.047
[epoch 35, batch   600] loss: 0.078
[epoch 35, batch   630] loss: 0.063
[epoch 35, batch   660] loss: 0.037
[epoch 35, batch   690] loss: 0.070
epoch 35 mean loss: 0.0609
[epoch 36, batch    30] loss: 0.039
[epoch 36, batch    60] loss: 0.060
[epoch 36, batch    90] loss: 0.054
[epoch 36, batch   120] loss: 0.019
[epoch 36, batch   150] loss: 0.017
[epoch 36, batch   180] loss: 0.023
[epoch 36, batch   210] loss: 0.011
[epoch 36, batch   240] loss: 0.034
[epoch 36, batch   270] loss: 0.030
[epoch 36, batch   300] loss: 0.015
[epoch 36, batch   330] loss: 0.033
[epoch 36, batch   360] loss: 0.026
[epoch 36, batch   390] loss: 0.038
[epoch 36, batch   420] loss: 0.024
[epoch 36, batch   450] loss: 0.017
[epoch 36, batch   480] loss: 0.046
[epoch 36, batch   510] loss: 0.014
[epoch 36, batch   540] loss: 0.040
[epoch 36, batch   570] loss: 0.022
[epoch 36, batch   600] loss: 0.034
[epoch 36, batch   630] loss: 0.045
[epoch 36, batch   660] loss: 0.026
[epoch 36, batch   690] loss: 0.012
epoch 36 mean loss: 0.0300
[epoch 37, batch    30] loss: 0.038
[epoch 37, batch    60] loss: 0.009
[epoch 37, batch    90] loss: 0.018
[epoch 37, batch   120] loss: 0.113
[epoch 37, batch   150] loss: 0.029
[epoch 37, batch   180] loss: 0.030
[epoch 37, batch   210] loss: 0.025
[epoch 37, batch   240] loss: 0.015
[epoch 37, batch   270] loss: 0.021
[epoch 37, batch   300] loss: 0.034
[epoch 37, batch   330] loss: 0.026
[epoch 37, batch   360] loss: 0.024
[epoch 37, batch   390] loss: 0.044
[epoch 37, batch   420] loss: 0.019
[epoch 37, batch   450] loss: 0.012
[epoch 37, batch   480] loss: 0.038
[epoch 37, batch   510] loss: 0.017
[epoch 37, batch   540] loss: 0.033
[epoch 37, batch   570] loss: 0.009
[epoch 37, batch   600] loss: 0.021
[epoch 37, batch   630] loss: 0.026
[epoch 37, batch   660] loss: 0.016
[epoch 37, batch   690] loss: 0.015
epoch 37 mean loss: 0.0275
[epoch 38, batch    30] loss: 0.018
[epoch 38, batch    60] loss: 0.030
[epoch 38, batch    90] loss: 0.029
[epoch 38, batch   120] loss: 0.012
[epoch 38, batch   150] loss: 0.014
[epoch 38, batch   180] loss: 0.011
[epoch 38, batch   210] loss: 0.036
[epoch 38, batch   240] loss: 0.009
[epoch 38, batch   270] loss: 0.020
[epoch 38, batch   300] loss: 0.008
[epoch 38, batch   330] loss: 0.028
[epoch 38, batch   360] loss: 0.017
[epoch 38, batch   390] loss: 0.008
[epoch 38, batch   420] loss: 0.020
[epoch 38, batch   450] loss: 0.030
[epoch 38, batch   480] loss: 0.008
[epoch 38, batch   510] loss: 0.005
[epoch 38, batch   540] loss: 0.025
[epoch 38, batch   570] loss: 0.025
[epoch 38, batch   600] loss: 0.040
[epoch 38, batch   630] loss: 0.050
[epoch 38, batch   660] loss: 0.025
[epoch 38, batch   690] loss: 0.019
epoch 38 mean loss: 0.0212
[epoch 39, batch    30] loss: 0.016
[epoch 39, batch    60] loss: 0.013
[epoch 39, batch    90] loss: 0.006
[epoch 39, batch   120] loss: 0.013
[epoch 39, batch   150] loss: 0.012
[epoch 39, batch   180] loss: 0.023
[epoch 39, batch   210] loss: 0.010
[epoch 39, batch   240] loss: 0.014
[epoch 39, batch   270] loss: 0.044
[epoch 39, batch   300] loss: 0.018
[epoch 39, batch   330] loss: 0.040
[epoch 39, batch   360] loss: 0.020
[epoch 39, batch   390] loss: 0.037
[epoch 39, batch   420] loss: 0.020
[epoch 39, batch   450] loss: 0.016
[epoch 39, batch   480] loss: 0.007
[epoch 39, batch   510] loss: 0.033
[epoch 39, batch   540] loss: 0.045
[epoch 39, batch   570] loss: 0.008
[epoch 39, batch   600] loss: 0.011
[epoch 39, batch   630] loss: 0.016
[epoch 39, batch   660] loss: 0.004
[epoch 39, batch   690] loss: 0.024
epoch 39 mean loss: 0.0194
[epoch 40, batch    30] loss: 0.020
[epoch 40, batch    60] loss: 0.016
[epoch 40, batch    90] loss: 0.008
[epoch 40, batch   120] loss: 0.009
[epoch 40, batch   150] loss: 0.015
[epoch 40, batch   180] loss: 0.033
[epoch 40, batch   210] loss: 0.022
[epoch 40, batch   240] loss: 0.002
[epoch 40, batch   270] loss: 0.012
[epoch 40, batch   300] loss: 0.009
[epoch 40, batch   330] loss: 0.020
[epoch 40, batch   360] loss: 0.013
[epoch 40, batch   390] loss: 0.031
[epoch 40, batch   420] loss: 0.016
[epoch 40, batch   450] loss: 0.047
[epoch 40, batch   480] loss: 0.044
[epoch 40, batch   510] loss: 0.026
[epoch 40, batch   540] loss: 0.005
[epoch 40, batch   570] loss: 0.019
[epoch 40, batch   600] loss: 0.028
[epoch 40, batch   630] loss: 0.043
[epoch 40, batch   660] loss: 0.025
[epoch 40, batch   690] loss: 0.035
epoch 40 mean loss: 0.0213
[epoch 41, batch    30] loss: 0.015
[epoch 41, batch    60] loss: 0.018
[epoch 41, batch    90] loss: 0.008
[epoch 41, batch   120] loss: 0.028
[epoch 41, batch   150] loss: 0.014
[epoch 41, batch   180] loss: 0.007
[epoch 41, batch   210] loss: 0.002
[epoch 41, batch   240] loss: 0.009
[epoch 41, batch   270] loss: 0.014
[epoch 41, batch   300] loss: 0.046
[epoch 41, batch   330] loss: 0.009
[epoch 41, batch   360] loss: 0.019
[epoch 41, batch   390] loss: 0.002
[epoch 41, batch   420] loss: 0.023
[epoch 41, batch   450] loss: 0.032
[epoch 41, batch   480] loss: 0.048
[epoch 41, batch   510] loss: 0.007
[epoch 41, batch   540] loss: 0.006
[epoch 41, batch   570] loss: 0.028
[epoch 41, batch   600] loss: 0.021
[epoch 41, batch   630] loss: 0.016
[epoch 41, batch   660] loss: 0.007
[epoch 41, batch   690] loss: 0.006
epoch 41 mean loss: 0.0166
[epoch 42, batch    30] loss: 0.025
[epoch 42, batch    60] loss: 0.008
[epoch 42, batch    90] loss: 0.003
[epoch 42, batch   120] loss: 0.011
[epoch 42, batch   150] loss: 0.004
[epoch 42, batch   180] loss: 0.025
[epoch 42, batch   210] loss: 0.025
[epoch 42, batch   240] loss: 0.015
[epoch 42, batch   270] loss: 0.010
[epoch 42, batch   300] loss: 0.004
[epoch 42, batch   330] loss: 0.012
[epoch 42, batch   360] loss: 0.046
[epoch 42, batch   390] loss: 0.006
[epoch 42, batch   420] loss: 0.032
[epoch 42, batch   450] loss: 0.024
[epoch 42, batch   480] loss: 0.019
[epoch 42, batch   510] loss: 0.007
[epoch 42, batch   540] loss: 0.049
[epoch 42, batch   570] loss: 0.026
[epoch 42, batch   600] loss: 0.019
[epoch 42, batch   630] loss: 0.018
[epoch 42, batch   660] loss: 0.026
[epoch 42, batch   690] loss: 0.020
epoch 42 mean loss: 0.0191
[epoch 43, batch    30] loss: 0.038
[epoch 43, batch    60] loss: 0.029
[epoch 43, batch    90] loss: 0.041
[epoch 43, batch   120] loss: 0.030
[epoch 43, batch   150] loss: 0.015
[epoch 43, batch   180] loss: 0.043
[epoch 43, batch   210] loss: 0.028
[epoch 43, batch   240] loss: 0.007
[epoch 43, batch   270] loss: 0.011
[epoch 43, batch   300] loss: 0.040
[epoch 43, batch   330] loss: 0.018
[epoch 43, batch   360] loss: 0.028
[epoch 43, batch   390] loss: 0.050
[epoch 43, batch   420] loss: 0.014
[epoch 43, batch   450] loss: 0.008
[epoch 43, batch   480] loss: 0.015
[epoch 43, batch   510] loss: 0.001
[epoch 43, batch   540] loss: 0.021
[epoch 43, batch   570] loss: 0.023
[epoch 43, batch   600] loss: 0.015
[epoch 43, batch   630] loss: 0.047
[epoch 43, batch   660] loss: 0.016
[epoch 43, batch   690] loss: 0.011
epoch 43 mean loss: 0.0236
[epoch 44, batch    30] loss: 0.047
[epoch 44, batch    60] loss: 0.027
[epoch 44, batch    90] loss: 0.013
[epoch 44, batch   120] loss: 0.026
[epoch 44, batch   150] loss: 0.024
[epoch 44, batch   180] loss: 0.037
[epoch 44, batch   210] loss: 0.014
[epoch 44, batch   240] loss: 0.010
[epoch 44, batch   270] loss: 0.023
[epoch 44, batch   300] loss: 0.023
[epoch 44, batch   330] loss: 0.011
[epoch 44, batch   360] loss: 0.024
[epoch 44, batch   390] loss: 0.024
[epoch 44, batch   420] loss: 0.002
[epoch 44, batch   450] loss: 0.030
[epoch 44, batch   480] loss: 0.026
[epoch 44, batch   510] loss: 0.068
[epoch 44, batch   540] loss: 0.022
[epoch 44, batch   570] loss: 0.020
[epoch 44, batch   600] loss: 0.011
[epoch 44, batch   630] loss: 0.018
[epoch 44, batch   660] loss: 0.014
[epoch 44, batch   690] loss: 0.003
epoch 44 mean loss: 0.0221
[epoch 45, batch    30] loss: 0.026
[epoch 45, batch    60] loss: 0.008
[epoch 45, batch    90] loss: 0.003
[epoch 45, batch   120] loss: 0.024
[epoch 45, batch   150] loss: 0.011
[epoch 45, batch   180] loss: 0.008
[epoch 45, batch   210] loss: 0.009
[epoch 45, batch   240] loss: 0.006
[epoch 45, batch   270] loss: 0.040
[epoch 45, batch   300] loss: 0.013
[epoch 45, batch   330] loss: 0.030
[epoch 45, batch   360] loss: 0.007
[epoch 45, batch   390] loss: 0.034
[epoch 45, batch   420] loss: 0.001
[epoch 45, batch   450] loss: 0.015
[epoch 45, batch   480] loss: 0.013
[epoch 45, batch   510] loss: 0.002
[epoch 45, batch   540] loss: 0.002
[epoch 45, batch   570] loss: 0.011
[epoch 45, batch   600] loss: 0.006
[epoch 45, batch   630] loss: 0.014
[epoch 45, batch   660] loss: 0.014
[epoch 45, batch   690] loss: 0.007
epoch 45 mean loss: 0.0135
[epoch 46, batch    30] loss: 0.003
[epoch 46, batch    60] loss: 0.014
[epoch 46, batch    90] loss: 0.039
[epoch 46, batch   120] loss: 0.019
[epoch 46, batch   150] loss: 0.009
[epoch 46, batch   180] loss: 0.013
[epoch 46, batch   210] loss: 0.032
[epoch 46, batch   240] loss: 0.028
[epoch 46, batch   270] loss: 0.004
[epoch 46, batch   300] loss: 0.001
[epoch 46, batch   330] loss: 0.006
[epoch 46, batch   360] loss: 0.022
[epoch 46, batch   390] loss: 0.005
[epoch 46, batch   420] loss: 0.003
[epoch 46, batch   450] loss: 0.018
[epoch 46, batch   480] loss: 0.013
[epoch 46, batch   510] loss: 0.004
[epoch 46, batch   540] loss: 0.013
[epoch 46, batch   570] loss: 0.006
[epoch 46, batch   600] loss: 0.027
[epoch 46, batch   630] loss: 0.008
[epoch 46, batch   660] loss: 0.001
[epoch 46, batch   690] loss: 0.022
epoch 46 mean loss: 0.0132
[epoch 47, batch    30] loss: 0.011
[epoch 47, batch    60] loss: 0.006
[epoch 47, batch    90] loss: 0.007
[epoch 47, batch   120] loss: 0.006
[epoch 47, batch   150] loss: 0.002
[epoch 47, batch   180] loss: 0.020
[epoch 47, batch   210] loss: 0.011
[epoch 47, batch   240] loss: 0.006
[epoch 47, batch   270] loss: 0.009
[epoch 47, batch   300] loss: 0.006
[epoch 47, batch   330] loss: 0.020
[epoch 47, batch   360] loss: 0.002
[epoch 47, batch   390] loss: 0.006
[epoch 47, batch   420] loss: 0.000
[epoch 47, batch   450] loss: 0.001
[epoch 47, batch   480] loss: 0.018
[epoch 47, batch   510] loss: 0.005
[epoch 47, batch   540] loss: 0.010
[epoch 47, batch   570] loss: 0.014
[epoch 47, batch   600] loss: 0.008
[epoch 47, batch   630] loss: 0.003
[epoch 47, batch   660] loss: 0.008
[epoch 47, batch   690] loss: 0.014
epoch 47 mean loss: 0.0082
[epoch 48, batch    30] loss: 0.013
[epoch 48, batch    60] loss: 0.003
[epoch 48, batch    90] loss: 0.009
[epoch 48, batch   120] loss: 0.008
[epoch 48, batch   150] loss: 0.009
[epoch 48, batch   180] loss: 0.005
[epoch 48, batch   210] loss: 0.007
[epoch 48, batch   240] loss: 0.005
[epoch 48, batch   270] loss: 0.014
[epoch 48, batch   300] loss: 0.002
[epoch 48, batch   330] loss: 0.014
[epoch 48, batch   360] loss: 0.012
[epoch 48, batch   390] loss: 0.012
[epoch 48, batch   420] loss: 0.002
[epoch 48, batch   450] loss: 0.007
[epoch 48, batch   480] loss: 0.001
[epoch 48, batch   510] loss: 0.002
[epoch 48, batch   540] loss: 0.006
[epoch 48, batch   570] loss: 0.008
[epoch 48, batch   600] loss: 0.012
[epoch 48, batch   630] loss: 0.003
[epoch 48, batch   660] loss: 0.007
[epoch 48, batch   690] loss: 0.022
epoch 48 mean loss: 0.0086
[epoch 49, batch    30] loss: 0.024
[epoch 49, batch    60] loss: 0.007
[epoch 49, batch    90] loss: 0.002
[epoch 49, batch   120] loss: 0.002
[epoch 49, batch   150] loss: 0.001
[epoch 49, batch   180] loss: 0.003
[epoch 49, batch   210] loss: 0.018
[epoch 49, batch   240] loss: 0.006
[epoch 49, batch   270] loss: 0.002
[epoch 49, batch   300] loss: 0.011
[epoch 49, batch   330] loss: 0.004
[epoch 49, batch   360] loss: 0.001
[epoch 49, batch   390] loss: 0.002
[epoch 49, batch   420] loss: 0.007
[epoch 49, batch   450] loss: 0.007
[epoch 49, batch   480] loss: 0.001
[epoch 49, batch   510] loss: 0.017
[epoch 49, batch   540] loss: 0.014
[epoch 49, batch   570] loss: 0.028
[epoch 49, batch   600] loss: 0.001
[epoch 49, batch   630] loss: 0.005
[epoch 49, batch   660] loss: 0.022
[epoch 49, batch   690] loss: 0.009
epoch 49 mean loss: 0.0086
[epoch 50, batch    30] loss: 0.004
[epoch 50, batch    60] loss: 0.008
[epoch 50, batch    90] loss: 0.009
[epoch 50, batch   120] loss: 0.003
[epoch 50, batch   150] loss: 0.017
[epoch 50, batch   180] loss: 0.006
[epoch 50, batch   210] loss: 0.006
[epoch 50, batch   240] loss: 0.005
[epoch 50, batch   270] loss: 0.030
[epoch 50, batch   300] loss: 0.010
[epoch 50, batch   330] loss: 0.003
[epoch 50, batch   360] loss: 0.005
[epoch 50, batch   390] loss: 0.004
[epoch 50, batch   420] loss: 0.006
[epoch 50, batch   450] loss: 0.016
[epoch 50, batch   480] loss: 0.005
[epoch 50, batch   510] loss: 0.031
[epoch 50, batch   540] loss: 0.008
[epoch 50, batch   570] loss: 0.021
[epoch 50, batch   600] loss: 0.007
[epoch 50, batch   630] loss: 0.003
[epoch 50, batch   660] loss: 0.008
[epoch 50, batch   690] loss: 0.025
epoch 50 mean loss: 0.0102
Training Completed.
Debiasing for 2 epochs.
[epoch 1, batch    30] loss: 0.015
[epoch 1, batch    60] loss: 0.013
[epoch 1, batch    90] loss: 0.010
[epoch 1, batch   120] loss: 0.007
[epoch 1, batch   150] loss: 0.006
[epoch 1, batch   180] loss: 0.008
[epoch 1, batch   210] loss: 0.021
[epoch 1, batch   240] loss: 0.005
[epoch 1, batch   270] loss: 0.005
[epoch 1, batch   300] loss: 0.008
[epoch 1, batch   330] loss: 0.015
[epoch 1, batch   360] loss: 0.003
[epoch 1, batch   390] loss: 0.004
[epoch 1, batch   420] loss: 0.007
[epoch 1, batch   450] loss: 0.008
[epoch 1, batch   480] loss: 0.001
[epoch 1, batch   510] loss: 0.000
[epoch 1, batch   540] loss: 0.003
[epoch 1, batch   570] loss: 0.010
[epoch 1, batch   600] loss: 0.008
[epoch 1, batch   630] loss: 0.012
[epoch 1, batch   660] loss: 0.022
[epoch 1, batch   690] loss: 0.010
epoch 1 mean loss: 0.0088
/home/zl22853/code/GeoBS/baselines/main.py:210: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)
  lats, lons = np.radians(loc_te[:, 1]), np.radians(loc_te[:, 0])
[epoch 2, batch    30] loss: 0.004
[epoch 2, batch    60] loss: 0.004
[epoch 2, batch    90] loss: 0.002
[epoch 2, batch   120] loss: 0.001
[epoch 2, batch   150] loss: 0.003
[epoch 2, batch   180] loss: 0.002
[epoch 2, batch   210] loss: 0.022
[epoch 2, batch   240] loss: 0.006
[epoch 2, batch   270] loss: 0.001
[epoch 2, batch   300] loss: 0.013
[epoch 2, batch   330] loss: 0.001
[epoch 2, batch   360] loss: 0.000
[epoch 2, batch   390] loss: 0.001
[epoch 2, batch   420] loss: 0.007
[epoch 2, batch   450] loss: 0.004
[epoch 2, batch   480] loss: 0.008
[epoch 2, batch   510] loss: 0.001
[epoch 2, batch   540] loss: 0.004
[epoch 2, batch   570] loss: 0.001
[epoch 2, batch   600] loss: 0.001
[epoch 2, batch   630] loss: 0.010
[epoch 2, batch   660] loss: 0.005
[epoch 2, batch   690] loss: 0.016
epoch 2 mean loss: 0.0049
Debiasing Completed.
Model saved as TorchSpatial/pre_trained_models/space2vec-grid/model_nabirds_ebird_meta_Space2Vec-grid_trained50_debiased2.pth.tar; in total, trained for 50 epochs, debiased for 2 epochs, in the order of [('train', 50), ('debias', 2)]
Top-1 Accuracy on 24411.0 test images: 75.65%
Top-3 Accuracy on 24411.0 test images: 87.74%
MRR on 24411.0 test images: 0.8248
GBS score on 21045 valid test neighborhoods: 0.1281
