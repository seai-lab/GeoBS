/home/zl22853/code/GeoBS/baselines/main.py:149: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)
  lats, lons = np.radians(loc_tr[:,1]), np.radians(loc_tr[:,0])
GPU Name: NVIDIA RTX A6000
Loading nabirds_with_loc_2019.json - train
   using meta data: ebird_meta
	 22819 total entries
	 22819 entries with images
	 22599 entries with meta data
Loading nabirds_with_loc_2019.json - test
   using meta data: ebird_meta
	 24633 total entries
	 24633 entries with images
	 24411 entries with meta data
Check the radian of input data! tensor([-117,   33])
Training for 50 epochs.
[epoch 1, batch    30] loss: 6.302
[epoch 1, batch    60] loss: 6.126
[epoch 1, batch    90] loss: 5.401
[epoch 1, batch   120] loss: 4.311
[epoch 1, batch   150] loss: 2.778
[epoch 1, batch   180] loss: 1.941
[epoch 1, batch   210] loss: 1.601
[epoch 1, batch   240] loss: 1.367
[epoch 1, batch   270] loss: 1.067
[epoch 1, batch   300] loss: 0.969
[epoch 1, batch   330] loss: 0.980
[epoch 1, batch   360] loss: 0.876
[epoch 1, batch   390] loss: 0.783
[epoch 1, batch   420] loss: 0.787
[epoch 1, batch   450] loss: 0.717
[epoch 1, batch   480] loss: 0.796
[epoch 1, batch   510] loss: 0.787
[epoch 1, batch   540] loss: 0.681
[epoch 1, batch   570] loss: 0.687
[epoch 1, batch   600] loss: 0.765
[epoch 1, batch   630] loss: 0.709
[epoch 1, batch   660] loss: 0.626
[epoch 1, batch   690] loss: 0.672
epoch 1 mean loss: 1.7859
[epoch 2, batch    30] loss: 0.442
[epoch 2, batch    60] loss: 0.516
[epoch 2, batch    90] loss: 0.456
[epoch 2, batch   120] loss: 0.398
[epoch 2, batch   150] loss: 0.476
[epoch 2, batch   180] loss: 0.456
[epoch 2, batch   210] loss: 0.510
[epoch 2, batch   240] loss: 0.478
[epoch 2, batch   270] loss: 0.474
[epoch 2, batch   300] loss: 0.444
[epoch 2, batch   330] loss: 0.438
[epoch 2, batch   360] loss: 0.513
[epoch 2, batch   390] loss: 0.447
[epoch 2, batch   420] loss: 0.495
[epoch 2, batch   450] loss: 0.406
[epoch 2, batch   480] loss: 0.481
[epoch 2, batch   510] loss: 0.407
[epoch 2, batch   540] loss: 0.457
[epoch 2, batch   570] loss: 0.413
[epoch 2, batch   600] loss: 0.395
[epoch 2, batch   630] loss: 0.324
[epoch 2, batch   660] loss: 0.431
[epoch 2, batch   690] loss: 0.425
epoch 2 mean loss: 0.4477
[epoch 3, batch    30] loss: 0.272
[epoch 3, batch    60] loss: 0.374
[epoch 3, batch    90] loss: 0.365
[epoch 3, batch   120] loss: 0.304
[epoch 3, batch   150] loss: 0.329
[epoch 3, batch   180] loss: 0.352
[epoch 3, batch   210] loss: 0.299
[epoch 3, batch   240] loss: 0.369
[epoch 3, batch   270] loss: 0.304
[epoch 3, batch   300] loss: 0.287
[epoch 3, batch   330] loss: 0.325
[epoch 3, batch   360] loss: 0.350
[epoch 3, batch   390] loss: 0.343
[epoch 3, batch   420] loss: 0.305
[epoch 3, batch   450] loss: 0.334
[epoch 3, batch   480] loss: 0.335
[epoch 3, batch   510] loss: 0.347
[epoch 3, batch   540] loss: 0.300
[epoch 3, batch   570] loss: 0.321
[epoch 3, batch   600] loss: 0.308
[epoch 3, batch   630] loss: 0.327
[epoch 3, batch   660] loss: 0.295
[epoch 3, batch   690] loss: 0.322
epoch 3 mean loss: 0.3266
[epoch 4, batch    30] loss: 0.256
[epoch 4, batch    60] loss: 0.209
[epoch 4, batch    90] loss: 0.255
[epoch 4, batch   120] loss: 0.258
[epoch 4, batch   150] loss: 0.223
[epoch 4, batch   180] loss: 0.285
[epoch 4, batch   210] loss: 0.229
[epoch 4, batch   240] loss: 0.233
[epoch 4, batch   270] loss: 0.278
[epoch 4, batch   300] loss: 0.185
[epoch 4, batch   330] loss: 0.244
[epoch 4, batch   360] loss: 0.277
[epoch 4, batch   390] loss: 0.223
[epoch 4, batch   420] loss: 0.237
[epoch 4, batch   450] loss: 0.273
[epoch 4, batch   480] loss: 0.242
[epoch 4, batch   510] loss: 0.255
[epoch 4, batch   540] loss: 0.214
[epoch 4, batch   570] loss: 0.262
[epoch 4, batch   600] loss: 0.351
[epoch 4, batch   630] loss: 0.230
[epoch 4, batch   660] loss: 0.256
[epoch 4, batch   690] loss: 0.300
epoch 4 mean loss: 0.2508
[epoch 5, batch    30] loss: 0.121
[epoch 5, batch    60] loss: 0.160
[epoch 5, batch    90] loss: 0.271
[epoch 5, batch   120] loss: 0.169
[epoch 5, batch   150] loss: 0.192
[epoch 5, batch   180] loss: 0.202
[epoch 5, batch   210] loss: 0.196
[epoch 5, batch   240] loss: 0.182
[epoch 5, batch   270] loss: 0.215
[epoch 5, batch   300] loss: 0.228
[epoch 5, batch   330] loss: 0.194
[epoch 5, batch   360] loss: 0.197
[epoch 5, batch   390] loss: 0.186
[epoch 5, batch   420] loss: 0.171
[epoch 5, batch   450] loss: 0.183
[epoch 5, batch   480] loss: 0.203
[epoch 5, batch   510] loss: 0.201
[epoch 5, batch   540] loss: 0.203
[epoch 5, batch   570] loss: 0.161
[epoch 5, batch   600] loss: 0.223
[epoch 5, batch   630] loss: 0.220
[epoch 5, batch   660] loss: 0.199
[epoch 5, batch   690] loss: 0.246
epoch 5 mean loss: 0.1968
[epoch 6, batch    30] loss: 0.204
[epoch 6, batch    60] loss: 0.173
[epoch 6, batch    90] loss: 0.200
[epoch 6, batch   120] loss: 0.132
[epoch 6, batch   150] loss: 0.125
[epoch 6, batch   180] loss: 0.176
[epoch 6, batch   210] loss: 0.161
[epoch 6, batch   240] loss: 0.191
[epoch 6, batch   270] loss: 0.156
[epoch 6, batch   300] loss: 0.172
[epoch 6, batch   330] loss: 0.169
[epoch 6, batch   360] loss: 0.112
[epoch 6, batch   390] loss: 0.132
[epoch 6, batch   420] loss: 0.132
[epoch 6, batch   450] loss: 0.207
[epoch 6, batch   480] loss: 0.184
[epoch 6, batch   510] loss: 0.142
[epoch 6, batch   540] loss: 0.154
[epoch 6, batch   570] loss: 0.164
[epoch 6, batch   600] loss: 0.214
[epoch 6, batch   630] loss: 0.148
[epoch 6, batch   660] loss: 0.167
[epoch 6, batch   690] loss: 0.157
epoch 6 mean loss: 0.1635
[epoch 7, batch    30] loss: 0.124
[epoch 7, batch    60] loss: 0.151
[epoch 7, batch    90] loss: 0.160
[epoch 7, batch   120] loss: 0.131
[epoch 7, batch   150] loss: 0.115
[epoch 7, batch   180] loss: 0.100
[epoch 7, batch   210] loss: 0.121
[epoch 7, batch   240] loss: 0.127
[epoch 7, batch   270] loss: 0.131
[epoch 7, batch   300] loss: 0.164
[epoch 7, batch   330] loss: 0.168
[epoch 7, batch   360] loss: 0.155
[epoch 7, batch   390] loss: 0.178
[epoch 7, batch   420] loss: 0.141
[epoch 7, batch   450] loss: 0.128
[epoch 7, batch   480] loss: 0.118
[epoch 7, batch   510] loss: 0.164
[epoch 7, batch   540] loss: 0.135
[epoch 7, batch   570] loss: 0.156
[epoch 7, batch   600] loss: 0.158
[epoch 7, batch   630] loss: 0.130
[epoch 7, batch   660] loss: 0.118
[epoch 7, batch   690] loss: 0.186
epoch 7 mean loss: 0.1411
[epoch 8, batch    30] loss: 0.109
[epoch 8, batch    60] loss: 0.075
[epoch 8, batch    90] loss: 0.123
[epoch 8, batch   120] loss: 0.132
[epoch 8, batch   150] loss: 0.161
[epoch 8, batch   180] loss: 0.128
[epoch 8, batch   210] loss: 0.094
[epoch 8, batch   240] loss: 0.138
[epoch 8, batch   270] loss: 0.128
[epoch 8, batch   300] loss: 0.138
[epoch 8, batch   330] loss: 0.149
[epoch 8, batch   360] loss: 0.177
[epoch 8, batch   390] loss: 0.108
[epoch 8, batch   420] loss: 0.134
[epoch 8, batch   450] loss: 0.094
[epoch 8, batch   480] loss: 0.128
[epoch 8, batch   510] loss: 0.122
[epoch 8, batch   540] loss: 0.110
[epoch 8, batch   570] loss: 0.133
[epoch 8, batch   600] loss: 0.180
[epoch 8, batch   630] loss: 0.122
[epoch 8, batch   660] loss: 0.150
[epoch 8, batch   690] loss: 0.145
epoch 8 mean loss: 0.1329
[epoch 9, batch    30] loss: 0.101
[epoch 9, batch    60] loss: 0.137
[epoch 9, batch    90] loss: 0.100
[epoch 9, batch   120] loss: 0.089
[epoch 9, batch   150] loss: 0.122
[epoch 9, batch   180] loss: 0.109
[epoch 9, batch   210] loss: 0.085
[epoch 9, batch   240] loss: 0.105
[epoch 9, batch   270] loss: 0.136
[epoch 9, batch   300] loss: 0.098
[epoch 9, batch   330] loss: 0.112
[epoch 9, batch   360] loss: 0.110
[epoch 9, batch   390] loss: 0.153
[epoch 9, batch   420] loss: 0.122
[epoch 9, batch   450] loss: 0.139
[epoch 9, batch   480] loss: 0.093
[epoch 9, batch   510] loss: 0.134
[epoch 9, batch   540] loss: 0.148
[epoch 9, batch   570] loss: 0.149
[epoch 9, batch   600] loss: 0.107
[epoch 9, batch   630] loss: 0.114
[epoch 9, batch   660] loss: 0.098
[epoch 9, batch   690] loss: 0.109
epoch 9 mean loss: 0.1161
[epoch 10, batch    30] loss: 0.072
[epoch 10, batch    60] loss: 0.112
[epoch 10, batch    90] loss: 0.131
[epoch 10, batch   120] loss: 0.139
[epoch 10, batch   150] loss: 0.090
[epoch 10, batch   180] loss: 0.086
[epoch 10, batch   210] loss: 0.103
[epoch 10, batch   240] loss: 0.095
[epoch 10, batch   270] loss: 0.110
[epoch 10, batch   300] loss: 0.126
[epoch 10, batch   330] loss: 0.077
[epoch 10, batch   360] loss: 0.151
[epoch 10, batch   390] loss: 0.129
[epoch 10, batch   420] loss: 0.126
[epoch 10, batch   450] loss: 0.092
[epoch 10, batch   480] loss: 0.083
[epoch 10, batch   510] loss: 0.114
[epoch 10, batch   540] loss: 0.083
[epoch 10, batch   570] loss: 0.118
[epoch 10, batch   600] loss: 0.098
[epoch 10, batch   630] loss: 0.082
[epoch 10, batch   660] loss: 0.135
[epoch 10, batch   690] loss: 0.132
epoch 10 mean loss: 0.1081
